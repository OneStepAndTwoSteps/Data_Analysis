{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 十二、训练深度神经网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Batch Normalization](https://zhuanlan.zhihu.com/p/38176412)\n",
    "\n",
    "尽管使用 He 初始化和 ELU（或任何 ReLU 变体）可以显著减少训练开始阶段的梯度消失/爆炸问题，但不能保证在训练期间问题不会再次出现。\n",
    "\n",
    "[在 2015 年的一篇论文中](https://links.jianshu.com/go?to=https%3A%2F%2Farxiv.org%2Fabs%2F1502.03167)，Sergey Ioffe 和 Christian Szegedy 提出了一种称为批归一化（Batch Normalization，BN）的方法来解决梯度消失/爆炸问题。该方法包括在每层的激活函数之前或之后在模型中添加操作。操作就是将输入平均值变为 0，方差变为 1，然后用两个新参数，一个做缩放，一个做偏移。换句话说，这个操作可以让模型学习到每层输入值的最佳缩放值和平均值。大大多数情况下，如果模型的第一层使用了 BN 层，则不用标准化训练集（比如使用`StandardScaler`）；BN 层做了标准化工作（虽然是近似的，每次每次只处理一个批次，但能做缩放和平移）。\n",
    "\n",
    "为了对输入进行零居中（平均值是 0）和归一化，算法需要估计输入的均值和标准差。 它通过评估当前小批量输入的均值和标准差（因此命名为“批归一化”）来实现。 整个操作在公式中。\n",
    "\n",
    "\n",
    "<div align=center><img width=\"500\" height=\"300\" src=\"./static/1.jpg\"/></div>\n",
    "\n",
    "\n",
    "#### 注意：\n",
    "\n",
    "你可能会发现，训练相当缓慢，这是因为每个周期都因为使用 BN 而延长了时间。但是有了 BN，收敛的速度更快，需要的周期数更少。综合来看，需要的总时长变短了。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 使用 Keras 实现批归一化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28,28]),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dense(300,activation='elu',kernel_initializer='he_normal'),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dense(100,activation='elu',kernel_initializer='he_normal'),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dense(10,activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_1 (Flatten)          (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 784)               3136      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 300)               235500    \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 300)               1200      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 100)               30100     \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 100)               400       \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 10)                1010      \n",
      "=================================================================\n",
      "Total params: 271,346\n",
      "Trainable params: 268,978\n",
      "Non-trainable params: 2,368\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看到每个 BN 层添加了四个参数：γ、 β、 μ 和 σ（例如，第一个 BN 层添加了 3136 个参数，即4 × 784）。后两个参数μ 和 σ是移动平均，不受反向传播影响，Keras 称其“不可训练”（如果将 BN 的总参数3,136 + 1,200 + 400除以 2，得到 2368，就是模型中总的不可训练的参数量）。\n",
    "\n",
    "看下第一个 BN 层的参数。两个参数是可训练的（通过反向传播），两个不可训练："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('batch_normalization_2/gamma:0', True),\n",
       " ('batch_normalization_2/beta:0', True),\n",
       " ('batch_normalization_2/moving_mean:0', False),\n",
       " ('batch_normalization_2/moving_variance:0', False)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(var.name ,var.trainable) for var in model.layers[1].variables]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "当在 Keras 中创建一个 BN 层时，训练过程中，还会创建两个 Keras 在迭代时的操作。该操作会更新移动平均值。因为后端使用的是 TensorFlow，这些操作就是 TensorFlow 操作（第 12 章会讨论 TF 操作）："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\APYPLA~1\\venv_dir\\venv1\\lib\\site-packages\\keras\\engine\\base_layer.py:1348: UserWarning: `layer.updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  warnings.warn('`layer.updates` will be removed in a future version. '\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers[1].updates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BN 的论文作者建议在激活函数之前使用 BN 层，而不是像前面的例子添加到后面。到底是前面还是后面好存在争议，取决于具体的任务 —— 你最好在数据集上试验一下哪种选择好。要在激活函数前添加 BN 层，必须将激活函数从隐藏层拿出来，单独做成一层。另外，因为 BN 层对每个输入有一个偏移参数，可以将前一层的偏置项去掉（设置use_bias=False）："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[6]),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dense(300,kernel_initializer='he_normal',use_bias=False),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Activation('relu'),\n",
    "    keras.layers.Dense(100,kernel_initializer='he_normal',use_bias=False),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Activation('relu'),\n",
    "    keras.layers.Dense(10,activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 梯度裁剪"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "减少梯度爆炸问题的一种常用技术是在反向传播过程中剪切梯度，使它们不超过某个阈值，这种方法称为梯度裁剪。梯度裁剪在循环神经网络中用的很多，因为循环神经网络中用 BN 很麻烦，参见第 15 章。 对于其它类型的网络，BN 就足够了。在 Keras 中，梯度裁剪只需在创建优化器时设置clipvalue或clipnorm参数，如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.SGD(clipvalue=1.0)\n",
    "model.compile(loss='mse',optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "优化器会将梯度向量中的每个值裁剪到 -1.0 和 1.0 之间。这意味着损失（对每个可训练参数）的所有偏导数会被裁剪到 -1.0 和 1.0 之间。阈值是一个可以调节的超参数，可能影响到梯度向量的方向。例如，如果原始梯度向量是 `[0.9, 100.0]` ，它大体指向第二个轴；但在裁剪之后变为 `[0.9, 1.0]` ，方向就大体指向对角线了。在实际中，梯度裁剪的效果不错。如果想确保梯度裁剪不改变梯度向量的方向，就需要设置clipnorm靠范数裁剪，这样如果梯度的 l2 范数超过了阈值，就能对整个梯度裁剪。例如，如果设置 `clipnorm = 1.0` ，向量 `[0.9, 100.0]` 就会被裁剪为 `[0.00899964, 0.9999595]` ，方向没变，但第一个量几乎被抹去了。如果再训练过程中发现了梯度爆炸（可以用 TensorBoard 跟踪梯度），最好的方法是既用值也用范数裁剪，设置不同的阈值，看看哪个在验证集上表现最好。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 复用预训练层"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从零开始训练一个非常大的 DNN 通常不是一个好主意，相反，您应该总是尝试找到一个现有的神经网络来完成与您正在尝试解决的任务类似的任务（第 14 章会介绍如何找），然后复用这个网络的较低层：这就是所谓的迁移学习。这样不仅能大大加快训练速度，还将需要更少的训练数据。\n",
    "\n",
    "例如，假设你有一个经过训练的 DNN，能将图片分为 100 个不同的类别，包括动物，植物，车辆和日常物品。 现在想要训练一个 DNN 来对特定类型的车辆进行分类。 这些任务非常相似，甚至部分重叠，因此应该尝试重新使用第一个网络的一部分（请参见图 ）。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=center><img width=\"500\" height=\"300\" src=\"./static/2.jpg\"/></div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 笔记：\n",
    "如果新任务的输入图像与原始任务中使用的输入图像的大小不一致，则必须添加预处理步骤以将其大小调整为原始模型的预期大小。 更一般地说，如果输入具有类似的低级层次的特征，则迁移学习将很好地工作。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "原始模型的输出层通常要替换掉，因为对于新任务可能一点用也没有，输出的数量可能就不对。相似的，原始模型的上层也不如浅层管用，因为高阶特征可能相差很大。需要确定好到底用几层。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 提示：\n",
    "\n",
    "任务越相似，可复用的层越多。对于非常相似的任务，可以尝试保留所有的隐藏层，替换输出层。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "先将所有复用的层冻结（即，使其权重不可训练，梯度下降不能修改权重），然后训练模型，看其表现如何。然后将复用的最上一或两层解冻，让反向传播可以调节它们，再查看性能有无提升。训练数据越多，可以解冻的层越多。解冻时减小学习率也有帮助，可以避免破坏微调而得的权重。\n",
    "\n",
    "如果效果不好，或者训练数据不多，可以尝试去除顶层，将其余的层都解冻。不断尝试，直到找到合适的层，如果训练数据很多，可以尝试替换顶层，或者加入更多的隐藏层。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 用 Keras 进行迁移学习"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "看一个例子。假设 Fashion MNIST 只有八个类，不包括拖鞋和 T 恤。一些人在这个数据集上搭建并训练了一个 Keras 模型，且效果不错（准确率大于 90%），将其称为模型 A。现在想处理另一个问题：有拖鞋和 T 恤的图片，要训练一个二分类器（`positive=shirt, negative=sandal`）。数据集不大，只有 200 张打了标签的图片。当训练架构与模型 A 相同的新模型时（称其为模型 B），表现非常好（准确率 97.2%）。但因为这是一个非常简单的任务（只有两类），所以准确率应该还可以更高。因为和任务 A 很像，所以可以尝试一下迁移学习。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "hoursing = fetch_california_housing()\n",
    "\n",
    "x_train_full,x_test,y_train_full,y_test = train_test_split(hoursing.data,hoursing.target)\n",
    "x_train,x_valid,y_train,y_valid = train_test_split(x_train_full,y_train_full) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss = StandardScaler()\n",
    "x_train = ss.fit_transform(x_train)\n",
    "x_valid = ss.transform(x_valid)\n",
    "x_test = ss.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_A,x_train_B = x_train[:,:5],x_train[:,2:]\n",
    "x_valid_A,x_valid_B = x_valid[:,:5],x_valid[:,2:]\n",
    "x_test_A,x_test_B = x_test[:,:5],x_test[:,2:]\n",
    "x_new_A,x_new_B = x_test_A[:3],x_test_B[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11610, 6)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_B.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "首先，加载模型 A，创建一个新模型，除了输出层不要，保留所有的层："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_A = keras.models.load_model('my_keras_model.h5')\n",
    "# model_A_on_B = keras.models.Sequential(model_A.layers[:-1])\n",
    "# model_A_on_B.add(keras.layers.Dense(1,activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model_A 和 model_B_on_A 公用了一些层。当你训练model_B_on_A时，也会影响model_A。如果想避免，需要在复用前克隆model_A。要这么做，可以使用clone.model()，然后复制权重（clone.model()不能克隆权重）："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_A_clone = keras.models.clone_model(model)\n",
    "model_A_clone.set_weights(model.get_weights()) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在就可以训练model_A了，但是因为新输出层是随机初始化的，误差较大，较大的误差梯度可能会破坏复用的权重。为了避免，一种方法是在前几次周期中，冻结复用的层，让新层有时间学到合理的权重。要实现的话，将每层的trainable属性设为False，然后编译模型："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in model_A_clone.layers[:-1]:\n",
    "    layer.trainable =False\n",
    "\n",
    "model_A_clone.compile(loss='mse',optimizer='sgd',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 笔记：\n",
    "\n",
    "冻结或解冻模型之后，都需要编译."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "训练几个周期之后，就可以解冻复用层（需要再次编译模型），然后接着训练以微调模型。解冻之后，最好降低学习率，目的还是避免破坏复用层的权重："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 5.2240 - accuracy: 0.0012 - val_loss: 5.1178 - val_accuracy: 7.7519e-04\n",
      "Epoch 2/4\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 5.2218 - accuracy: 0.0011 - val_loss: 5.1166 - val_accuracy: 7.7519e-04\n",
      "Epoch 3/4\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 5.2209 - accuracy: 0.0013 - val_loss: 5.1160 - val_accuracy: 7.7519e-04\n",
      "Epoch 4/4\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 5.2204 - accuracy: 0.0013 - val_loss: 5.1155 - val_accuracy: 7.7519e-04\n"
     ]
    }
   ],
   "source": [
    "history = model_A_clone.fit(x_train_B,y_train,epochs=4,validation_data=(x_valid_B,y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\APYPLA~1\\venv_dir\\venv1\\lib\\site-packages\\keras\\optimizer_v2\\optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/16\n",
      "363/363 [==============================] - 2s 2ms/step - loss: 5.2230 - accuracy: 6.8906e-04 - val_loss: 5.1151 - val_accuracy: 0.0016\n",
      "Epoch 2/16\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 5.2230 - accuracy: 9.4746e-04 - val_loss: 5.1151 - val_accuracy: 0.0010\n",
      "Epoch 3/16\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 5.2230 - accuracy: 6.0293e-04 - val_loss: 5.1155 - val_accuracy: 0.0013\n",
      "Epoch 4/16\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 5.2230 - accuracy: 8.6133e-04 - val_loss: 5.1153 - val_accuracy: 0.0013\n",
      "Epoch 5/16\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 5.2229 - accuracy: 6.8906e-04 - val_loss: 5.1156 - val_accuracy: 0.0010\n",
      "Epoch 6/16\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 5.2229 - accuracy: 6.0293e-04 - val_loss: 5.1151 - val_accuracy: 0.0013\n",
      "Epoch 7/16\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 5.2229 - accuracy: 6.8906e-04 - val_loss: 5.1153 - val_accuracy: 0.0013\n",
      "Epoch 8/16\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 5.2228 - accuracy: 6.0293e-04 - val_loss: 5.1155 - val_accuracy: 0.0010\n",
      "Epoch 9/16\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 5.2228 - accuracy: 6.0293e-04 - val_loss: 5.1154 - val_accuracy: 0.0013\n",
      "Epoch 10/16\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 5.2228 - accuracy: 7.7519e-04 - val_loss: 5.1151 - val_accuracy: 0.0016\n",
      "Epoch 11/16\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 5.2227 - accuracy: 6.8906e-04 - val_loss: 5.1153 - val_accuracy: 0.0013\n",
      "Epoch 12/16\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 5.2227 - accuracy: 6.8906e-04 - val_loss: 5.1154 - val_accuracy: 0.0013\n",
      "Epoch 13/16\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 5.2227 - accuracy: 6.8906e-04 - val_loss: 5.1152 - val_accuracy: 0.0013\n",
      "Epoch 14/16\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 5.2226 - accuracy: 8.6133e-04 - val_loss: 5.1154 - val_accuracy: 0.0013\n",
      "Epoch 15/16\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 5.2226 - accuracy: 6.8906e-04 - val_loss: 5.1153 - val_accuracy: 0.0013\n",
      "Epoch 16/16\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 5.2226 - accuracy: 4.3066e-04 - val_loss: 5.1156 - val_accuracy: 0.0010\n"
     ]
    }
   ],
   "source": [
    "for layer in model_A_clone.layers[:-1]:\n",
    "    layer.trainable = True\n",
    "\n",
    "optimizer = keras.optimizers.SGD(lr=1e-4) # the default lr is 1e-2\n",
    "model_A_clone.compile(loss=\"mse\", optimizer=optimizer,\n",
    "                     metrics=[\"accuracy\"])\n",
    "history = model_A_clone.fit(x_train_B, y_train, epochs=16,\n",
    "                           validation_data=(x_valid_B, y_valid)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "162/162 [==============================] - 0s 1ms/step - loss: 5.2551 - accuracy: 9.6899e-04\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[5.255092620849609, 0.0009689922444522381]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_A_clone.evaluate(x_test_B, y_test) # 前面是误差率，后面的是准确率"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 更快的优化器"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "训练一个非常大的深度神经网络可能会非常缓慢。 到目前为止，我们已经看到了四种加速训练的方法（并且达到更好性能的方法）：对连接权重应用良好的初始化策略，使用良好的激活函数，使用批归一化以及重用预训练网络的部分（使用辅助任务或无监督学习）。 另一个速度提升的方法是使用更快的优化器，而不是常规的梯度下降优化器。 在本节中，我们将介绍最流行的算法：动量优化，Nesterov 加速梯度，AdaGrad，RMSProp，最后是 Adam 和 Nadam 优化。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 剧透：\n",
    "\n",
    "本节的结论是，几乎总是应该使用 `Adam_optimization` ，所以如果不关心它是如何工作的，只需使用 `AdamOptimizer` 替换 `GradientDescentOptimizer` ，然后跳到下一节！ 只需要这么小的改动，训练通常会快几倍。 但是，`Adam 优化` 确实有三个可以调整的超参数（加上学习率）。 默认值通常工作的不错，但如果您需要调整它们，知道他们怎么实现的可能会有帮助。 `Adam 优化` 结合了来自其他优化算法的几个想法，所以先看看这些算法是有用的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv1",
   "language": "python",
   "name": "venv1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
