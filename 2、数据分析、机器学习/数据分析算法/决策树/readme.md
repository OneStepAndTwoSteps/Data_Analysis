# 决策树常用算法


*   __ID3__
    
    __支持模型：__ 分类
    
    __树结构：__ 多叉树
    
    __特征选择：__ 信息增益
    
    __连续值处理：__ 不支持
    
    __缺失值处理：__ 不支持
    
    __剪枝：__ 不支持
  
  
*   __C4.5__

    __支持模型：__ 分类
    
    __树结构：__ 多叉树
    
    __特征选择：__ 信息增益率
    
    __连续值处理：__ 支持
    
    __缺失值处理：__ 支持
    
    __剪枝：__ 支持
  
*   __CART__

    __支持模型：__ 分类、回归
    
    __树结构：__ 二叉数
    
    __特征选择：__ 基尼系数
    
    __连续值处理：__ 支持
    
    __缺失值处理：__ 支持
    
    __剪枝：__ 支持

### `gini 不纯度(Gini Impurity)`

* 基尼不纯度：Gini Impurity，也被称为基尼指数（Gini Index），用来构造决策树中的CART分类树

### `gini 系数(Gini Coefficient)`

* 早用来衡量收入分配差异，后来也用作分类模型性能评估

* [gini 系数和 auc 之间存在联系：Gini和AUC的关系](https://www.pianshen.com/article/675879763/)

* 业界在实际计算Gini系数时往往用ROC曲线曲线和中线围成的面积与中线之上面积的比例，也就是Gini=2AUC-1。


    <div align=center><img width="400" height="300" src="./static/gini_and_auc.png"/></div>

* 也就是说用ROC曲线去计算Gini的前提是ROC曲线和Gini曲线时重合的，因此Gini coefficient与AUC可以互相转换：

        gini = A / (A + B) = (AUC - C) / (A + B) = (AUC -0.5) / 0.5 = 2*AUC - 1

* 那问题来了，ROC曲线与Gini的洛伦兹曲线到底是不是重合的呢？

* 结论：当样本中坏样本极少时可用gini=2AUC-1近似计算，当坏样本较多，或者好坏样本接近1:1时，那就得对gini单独计算比较准确。




 ### `ID3、C4.5 和 CART 三者之间的差异：`

`划分标准的差异：`ID3 使用信息增益偏向特征值多的特征，C4.5 使用信息增益率克服信息增益的缺点，偏向于特征值小的特征，CART 使用基尼指数克服 C4.5 需要求 log 的巨大计算量，偏向于特征值较多的特征。

`使用场景的差异：`ID3 和 C4.5 都只能用于分类问题，CART 可以用于分类和回归问题；ID3 和 C4.5 是多叉树，速度较慢，CART 是二叉树，计算速度很快；

`样本数据的差异：`ID3 只能处理离散数据且缺失值敏感，C4.5 和 CART 可以处理连续性数据且有多种方式处理缺失值；从样本量考虑的话，小样本建议 C4.5、大样本建议 CART。C4.5 处理过程中需对数据集进行多次扫描排序，处理成本耗时较高，而 CART 本身是一种大样本的统计方法，小样本处理下泛化误差较大 ；

`样本特征的差异：`ID3 和 C4.5 层级之间只使用一次特征，CART 可多次重复使用特征；

`剪枝策略的差异：`ID3 没有剪枝策略，C4.5 是通过悲观剪枝策略来修正树的准确性，而 CART 是通过代价复杂度剪枝。


### 决策树的优缺点：

`决策树优点：`

1、是得到的模型很容易可视化，非专家也很容易理解。

2、算法完全不受数据缩放的影响。由于每个特征被单独处理，而且数据的划分也不依赖于缩放，因此决策树算法不需要特征预处理，如果归一化或标准化。特别是特征的尺度完全不一样时或者二元特征和连续特征同时存在时，决策树的效果很好。

3、可以处理缺失值（missing）， 字符型（nominal）， 数值型（numeric）等数据类型。

4、非参数模型（non-parametric）。 没有复杂的参数设置，谁跑效果都相对一样。

5、对相关（Correlation）属性能够比较好的处理。

6、运算速度相对比较快。


`缺点：`

最大的缺点就是很容易过拟合。 导致实际预测的效果并不高。

决策树中，重复的部分过多难以概括， 譬如对于 ( F1 && F2 ) || ( F3 && F4 ) 的表达（如下图，划圈的部分重复）， 决策树就有很大的重复。

不适合处理高维数据， 当属性数量过大的时候， 部分决策树就不太适用了。

对异常值（Outlier）过于敏感， 很容易导致树的结构的巨大的变换。
 
泛化（Generalization）能力太差， 对于没有出现过的值几乎没有办法。





