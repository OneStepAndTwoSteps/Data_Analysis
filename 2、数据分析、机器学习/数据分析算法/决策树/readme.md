# 决策树

决策树是一种树形结构，其中每个内部节点表示一个属性上的判断，每个分支代表一个判断结果的输出，最后每个叶节点代表一种分类结果。



## `一、决策树常用算法`

*   __`ID3`__
    
    __支持模型：__ 分类
    
    __树结构：__ 多叉树
    
    __特征选择：__ 信息增益
    
    __连续值处理：__ 不支持
    
    __缺失值处理：__ 不支持
    
    __剪枝：__ 不支持
  
*   `信息熵公式：`

<div align=center><img src="./static/信息熵.png"/></div>



*   `信息增益：`

<div align=center><img src="./static/信息增益公式.png"/></div>



*   __`C4.5`__

    __支持模型：__ 分类
    
    __树结构：__ 多叉树
    
    __特征选择：__ 信息增益率
    
    __连续值处理：__ 支持
    
    __缺失值处理：__ 支持
    
    __剪枝：__ 支持

*   `信息增益率: `


<div align=center><img src="./static/信息增益率公式.png"/></div>



*   __`CART`__

    __支持模型：__ 分类、回归
    
    __树结构：__ 二叉数
    
    __特征选择：__ 基尼系数
    
    __连续值处理：__ 支持
    
    __缺失值处理：__ 支持
    
    __剪枝：__ 支持


*   `gini 系数:`


<div align=center><img src="./static/gini公式.png"/></div>




### `gini 不纯度(Gini Impurity)`

* 基尼不纯度：Gini Impurity，也被称为基尼指数（Gini Index），用来构造决策树中的CART分类树

### `gini 系数(Gini Coefficient)`

* 早用来衡量收入分配差异，后来也用作分类模型性能评估

* [gini 系数和 auc 之间存在联系：Gini和AUC的关系](https://www.pianshen.com/article/675879763/)

* 业界在实际计算Gini系数时往往用ROC曲线曲线和中线围成的面积与中线之上面积的比例，也就是Gini=2AUC-1。


    <div align=center><img width="400" height="300" src="./static/gini_and_auc.png"/></div>

* 也就是说用ROC曲线去计算Gini的前提是ROC曲线和Gini曲线时重合的，因此Gini coefficient与AUC可以互相转换：

        gini = A / (A + B) = (AUC - C) / (A + B) = (AUC -0.5) / 0.5 = 2*AUC - 1

* 那问题来了，ROC曲线与Gini的洛伦兹曲线到底是不是重合的呢？

* 结论：当样本中坏样本极少时可用gini=2AUC-1近似计算，当坏样本较多，或者好坏样本接近1:1时，那就得对gini单独计算比较准确。


### `CART 树的二叉树如何划分：`


<div align=center><img src="./static/cart树划分.jpg"/></div>



 ### `ID3、C4.5 和 CART 三者之间的差异：`

* `划分标准的差异：`ID3 使用信息增益偏向特征值多的特征，C4.5 使用信息增益率克服信息增益的缺点，偏向于特征值小的特征，CART 使用基尼指数克服 C4.5 需要求 log 的巨大计算量，偏向于特征值较多的特征。

* `使用场景的差异：`ID3 和 C4.5 都只能用于分类问题，CART 可以用于分类和回归问题；ID3 和 C4.5 是多叉树，速度较慢，CART 是二叉树，计算速度很快；

* `样本数据的差异：`ID3 只能处理离散数据且缺失值敏感，C4.5 和 CART 可以处理连续性数据且有多种方式处理缺失值；从样本量考虑的话，小样本建议 C4.5、大样本建议 CART。C4.5 处理过程中需对数据集进行多次扫描排序，处理成本耗时较高，而 CART 本身是一种大样本的统计方法，小样本处理下泛化误差较大 ；

* `样本特征的差异：`ID3 和 C4.5 层级之间只使用一次特征，CART 可多次重复使用特征；

* `剪枝策略的差异：`ID3 没有剪枝策略，C4.5 是通过悲观剪枝策略来修正树的准确性，而 CART 是通过代价复杂度剪枝。


## `二、决策树的优缺点：`

`决策树优点：`

* 1、是得到的模型很容易可视化，非专家也很容易理解。

* 2、算法完全不受数据缩放的影响。由于每个特征被单独处理，而且数据的划分也不依赖于缩放，因此决策树算法不需要特征预处理，如果归一化或标准化。特别是特征的尺度完全不一样时或者二元特征和连续特征同时存在时，决策树的效果很好。

* 3、可以处理缺失值（missing）， 字符型（nominal）， 数值型（numeric）等数据类型。

* 4、非参数模型（non-parametric）。 没有复杂的参数设置，谁跑效果都相对一样。

* 5、对相关（Correlation）属性能够比较好的处理。

* 6、运算速度相对比较快。


`缺点：`

* 1、最大的缺点就是很容易过拟合。 导致实际预测的效果并不高。

* 2、决策树中，重复的部分过多难以概括， 譬如对于 ( F1 && F2 ) || ( F3 && F4 ) 的表达（如下图，划圈的部分重复）， 决策树就有很大的重复。

* 3、不适合处理高维数据， 当属性数量过大的时候， 部分决策树就不太适用了。

* 4、对异常值（Outlier）过于敏感， 很容易导致树的结构的巨大的变换。
    
* 5、泛化（Generalization）能力太差， 对于没有出现过的值几乎没有办法。

## `三、关于决策树剪枝、缺失值如何处理`


### `3.1、决策树如何处理缺失值`

在处理缺失值之前需要解决两个问题：

    1、如何在属性有缺失值之前进行属性划分选择？

    2、给定划分属性，若样本在该属性上任有缺失值，如何对样本进行划分

第一个问题：

    拿 ID3 算法举例，我们是按照信息增益划分属性，假如在判断一个 `西瓜是不是好西瓜` 的样本集（17个样本）中（色泽）有3个缺失值（14个样本没有缺失值），那我们按照没有缺失值的样本计算信息增益，只不过在计算信息增益乘以每个样本的权重的时候，权重的分母为未缺失的样本数量。

现有数据：

<div align=center><img src="./static/缺失值处理/数据.jpg"/></div>



    1、计算未缺失的样本中 xx 特征和 其中特征值 的信息熵：


<div align=center><img width="600" height="400" src="./static/缺失值处理/信息熵2.jpg
"/></div>
    

    2、计算信息增益：
    

<div align=center><img src="./static/缺失值处理/信息增益.jpg"/></div>


    3、如果其他特征也有缺失值，也按照这个方法来计算信息增益。

    4、然后比较各个特征之间的信息增益，信息增益最大的作为叶子节点。


第二个问题：

<div align=center><img src="./static/缺失值处理/第二个问题.png"/></div>













