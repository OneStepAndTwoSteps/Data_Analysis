# 线性回归

估计类条件概率的一种常见的策略就是先假定其具有某种确定的概率分布形式，再基于训练样本对概率分布的参数进行估计。

事实上概率模型的训练过程就是 `参数估计` 的过程。并且统计学的两个学派分别提供了两种不同的解决方案，这里先采用频率学派的极大似然估计法，`频率学派认为参数虽然未知，但是确实客观存在的固定值，因此可以通过优化似然函数等准则来确定参数值`

### 一、线性回归公式：
* <div align=center><img  src="./static/1.jpg"/></div>

### 二、预测值和真实值之间存在误差 ε ，如果要让预测值更准确，那么误差要越小越好

* <div align=center><img  src="./static/2.jpg"/></div>

### 三、机器学习中所有的内容都是基于假设，我们的数据必须是独立同分布的，不然没有意义。这里假设误差是基于标准正态分布，

* <div align=center><img  src="./static/3.jpg"/></div>

### 正态分布公式：

* <div align=center><img  src="./static/正态分布.jpg"/></div>

### 四、将误差带入到正态分布的公式中，因为误差服从标准正态分布，那么μ=0，σ=1，这里暂时先只带入μ，公式中的 exp 其实就是 e，紧接着通过 1 式将 ε 转成对应表达式带入 2 式子，化成只带有 θ 的表达式。



* <div align=center><img  src="./static/4.jpg"/></div>

### 五、因为我们想要误差越来越小，那么我们就希望我们最终的预测值越接近真实值越好，那么我们就看可以通过极大似然估计对式子进行求解。

*   [什么是极大似然估计？](https://www.zhihu.com/question/54082000)

* <div align=center><img  src="./static/5.jpg"/></div>

### 六、如果我们想要让logL(θ)尽可能大，那么B式就要尽可能小，B式其实就是最小二乘法的式子。其实一般线性回归我们直接用均方误差作为损失函数。损失函数的代数法表示如下：

* <div align=center><img  src="./static/6(2).jpg"/></div>

### 七、接下来 J(θ) 的极小值。

* <div align=center><img  src="./static/7.jpg"/></div>

## 其他参考资料

* https://www.cnblogs.com/pinard/p/6004041.html


