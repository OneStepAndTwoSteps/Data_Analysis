## 多项式回归

如果我们的数据集难以使用直线来进行拟合，但是令人惊讶的是，依然可以使用线性模型来拟合非线性数据。

一个简单的方法是对 __每个特征进行加权后作为新的特征__ ，然后训练一个线性模型在这个扩展的特征集。这种方法称为 __多项式回归__

我们使用 Scikit-Learning 的	PolynomialFeatures	类进行训练数据集的转换，让训练集中每个特征的平方（2	次多项式） 作为新特征（在这种情况下，仅存在一个特征）：

### PolynomialFeatures 说明

* __PolynomialFeatures__ 这个类有 3 个参数：

        degree：控制多项式的次数；

        interaction_only：默认为 False，如果指定为 True，那么就不会有特征自己和自己结合的项，组合的特征中没有 a2 和 b2；

        include_bias：默认为 True 。如果为 True 的话，那么结果中就会有 0 次幂项，即全为 1 这一列。

* __参数详解：__

        interaction_only 的意思是，得到的组合特征只有相乘的项，没有平方项。

        interaction_only 设置成 True 的意思是： 例如 [a,b] 的多项式交互式输出 [1,a,b,ab]。

        include_bias 设置 0 次幂那一列是否要。


### 案例

* __构造数据__

        x = np.random.uniform(-3, 3, size=100)
        X = x.reshape(-1, 1)
        noise = np.random.normal(0, 1, size=100)
        noise = noise.reshape(-1, 1)
        y = 0.5 * X**2 + X + 2 + noise


* __使用sklearn中的PolynomialFeatures进行多项式回归__

        from sklearn.preprocessing import PolynomialFeatures
        poly_features = PolynomialFeatures(degree=2,include_bias=False)
        x_poly = poly_features.fit_transform(X)

* __X_poly__ 现在 __包含原始特征__ 和 __这个特征的平方__ 。

        print('X[0]: ',X[0])
        print('x_poly[0]: ',x_poly[0])

        X[0]:  [0.7637525]
        x_poly[0]:  [0.7637525  0.58331788]

* 现在你可以在这个扩展训练集上使用 __LinearRegression__ 模型进行拟合。

        lin_reg2 = LinearRegression()
        lin_reg2.fit(x_poly,y)
        
        LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None, normalize=False) 

* __根据线性回归得到预测值__

        y_predect = lin_reg2.predict(x_poly)


### 使用多项式拟合数据 - 绘图
    
* __排序__ 是为了在绘制线的时候从最左端开始绘制

* __y_predect[np.argsort(x)]__ 依据 __np.sort(x)__ 的顺序

        plt.scatter(x,y)
        plt.plot(np.sort(x), y_predect[np.argsort(x)], color='r')

<div align=center><img width="450" height="350" src="https://raw.githubusercontent.com/OneStepAndTwoSteps/Data_Analysis_notes/master/static/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E7%AE%97%E6%B3%95/1.jpg"/></div>

