# 提升树 和 GBDT（梯度提升树）

## 简述


当损失函数是平方损失和指数函数时，每一步优化很简单，但对于一般损失函数而言，往往每一步优化都要不那么容易。所以出现了梯度提升树，梯度提升树利用最速下降法的近似方法，其关键是利用损失函数的负梯度在当前模型的值作为回归问题提升树算法中的残差的近似值，拟合一个回归树。

提升树和梯度提升树主要是在拟合损失上存在区别，一个是拟合残差（提升树）一个是使用负梯度（梯度提升树）作为损失，当GBDT使用最小二乘法作为损失函数时，GBDT 也变成拟合残差了。但是一般GBDT 不使用最小二乘法作为损失函数。
 


## `一、提升树`

### `1.1、提升树引出：`

* 1、提升树通过前向分布算法：

    <div align=center><img  src="./static/前向分布算法2.png"/></div>

* 2、可以看出我们的目标其实是优化损失函数

    <div align=center><img  src="./static/损失函数.png"/></div>

* 3、给定当前模型fm-1,需要求解：

    <div align=center><img  src="./static/最优参数θ.png"/></div>

    得到最优参数 θ，即 m 棵树的参数。

* 4、采用平方误差损失函数时：

    <div align=center><img  src="./static/均方误差损失.png"/></div>

* 5、简化成拟合残差

    <div align=center><img  src="./static/残差.png"/></div>


### `1.2、提升树 - 回归树案例：`

<div align=center><img  src="./static/提升回归树案例1.png"/></div>

`流程：`

* 1、设定分割点，按这个案例来说，会考虑如下分割点：

        1.5 2.5 3.5 4.5 5.5 6.5 7.5 8.5 9.5 

* 2、然后对数据进行分割，计算最小损失：

    `如当分割点为 1.5 时：`

        c1 = 5.56
        c2 = 7.50

        左节点损失：(5.56 - 5.56)² = 0

        右节点损失：(5.7 - 7.50)² + (5.91 - 7.50)² + ...... + (9.05 - 7.50)² = 15.72

        总损失 = 左节点损失 + 右节点损失 = 0 + 15.72 = 15.72

    `当分割点为 2.5 时: `


        按照如上方法计算，得到最后的总损失

        ......

    经过计算后发现以6.5为分割点得到的损失最小，那么 `回归树 T1` 为：

    <div align=center><img  src="./static/回归树T1_1.png"/></div>

* 3、接下来再进行学习的时候拟合的不再是原数据了，而是拟合残差：

    <div align=center><img  src="./static/残差表.png"/></div>

* 4、按照优化问题重新计算损失，选择最小的损失节点作为分割点,得到 `回归树T2` ：


    <div align=center><img  src="./static/回归树T2.png"/></div>

* 5、以此类推求得 T3、T4、T5、T6 .... 直到最后这些树足够拟合数据，残差达到你的满意值后停止训练。

* 6、如第六次停止训练后f6(x)为：

    <div align=center><img  src="./static/回归树f6.png"/></div>



## `二、GBDT 回归树`

`和提升树差不多，只不过拟合的不是残差，而是负梯度。`

* 残差只是负梯度的特例，最终的目的是使得损失函数减少的最快，沿哪个方向减少最快呢？负梯度方向。拟合负梯度，相当于损失函数沿着负梯度方向减少。当损失函数为平方损失时，拟合残差就相当于平方损失沿着负梯度方向减小。但是当损失函数不是平方损失时，再拟合残差就不是损失函数沿着负梯度减小了，所以此时当然不能再拟合残差。

`GBDT 的损失函数不是指建立回归树时候的拟合函数。`

回归树的拟合有自己的CART算法。这里的损失是指整个训练数据的损失函数。我们GBDT里的每一颗决策树拟合的输出y就是我们的“损失的拟合”，而我们的y是从我们GBDT损失函数的负梯度拟合得到的。

用前面的通俗例子来说就是：

* “假如有个人30岁，我们首先用20岁去拟合，发现损失有10岁，这时我们用6岁去拟合剩下的损失，发现差距还有4岁，第三轮我们用3岁拟合剩下的差距，差距就只有一岁了。如果我们的迭代轮数还没有完，可以继续迭代下面，每一轮迭代，拟合的岁数误差都会减小。”

* 我们GBDT第一棵决策树的y就是20，第二棵决策树的y就是6， 第三棵y是3.。。。而20,6,3都是通过损失函数负梯度求解得到的。


### `GBDT 拟合负梯度：`


<div align=center><img  src="./static/gbdt/gbdt的负梯度拟合.jpg"/></div>


`公式介绍：`

* I(x∈Rtj)的意思是如果x∈Rtj,则I的取值为1， 否则取值为0.

* 至于c，其实就是GBDT中每颗决策树拟合叶子节点最好的的输出值。在回归GBDT初始化的时候，c一般取值为所有y的平均值。




## `三、GBDT 分类树`

这里我们再看看GBDT分类算法，GBDT的分类算法从思想上和GBDT的回归算法没有区别，但是由于样本输出不是连续的值，而是离散的类别，导致我们无法直接从输出类别去拟合类别输出的误差。

为了解决这个问题，主要有两个方法，`一个是用指数损失函数，此时GBDT退化为Adaboost算法`。另一种方法是用类似于逻辑回归的对数似然损失函数的方法。也就是说，我们用的是类别的预测概率值和真实概率值的差来拟合损失。本文仅讨论用对数似然损失函数的GBDT分类。而对于对数似然损失函数，我们又有二元分类和多元分类的区别。







## 参考链接

* 梯度提升树(GBDT)原理小结： https://www.cnblogs.com/pinard/p/6140514.html


## commit

问：

    您好，
    请问如果不使用用 均方误差作为损失函数，那么每一轮拟合的就不再是残差。
    若果拟合的不是残差，那么每一轮结果相加的含义是什么呢？
    比如真实值为30，第一轮预测为20，第二轮用残差10去拟合，预测得到8，这样的两轮结果相加20+8很好理解
    如果使用负梯度去拟合，该如何理解呢？

答：

    你好，残差拟合是负梯度拟合的特殊情况。而负梯度你可以理解为下山，即某一位置判断得到的下山最近的方向，这样负梯度相加你可以理解为上一步判断得到的下山最近的方向，走一步，然后再判断得到的下山最近的方向，再走一步，这样循环的一个过程。

    当然，解释性就没有残差那么直接了。



