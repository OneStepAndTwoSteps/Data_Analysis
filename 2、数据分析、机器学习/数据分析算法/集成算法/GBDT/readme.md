# GBDT 梯度提升树

## `一、GBDT 回归树`




## `二、GBDT 分类树`

这里我们再看看GBDT分类算法，GBDT的分类算法从思想上和GBDT的回归算法没有区别，但是由于样本输出不是连续的值，而是离散的类别，导致我们无法直接从输出类别去拟合类别输出的误差。

为了解决这个问题，主要有两个方法，`一个是用指数损失函数，此时GBDT退化为Adaboost算法`。另一种方法是用类似于逻辑回归的对数似然损失函数的方法。也就是说，我们用的是类别的预测概率值和真实概率值的差来拟合损失。本文仅讨论用对数似然损失函数的GBDT分类。而对于对数似然损失函数，我们又有二元分类和多元分类的区别。





## 参考链接

* 梯度提升树(GBDT)原理小结： https://www.cnblogs.com/pinard/p/6140514.html