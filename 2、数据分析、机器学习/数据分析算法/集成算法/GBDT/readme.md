# 提升树 和 GBDT（梯度提升树）

## 简述


当损失函数是平方损失和指数函数时，每一步优化很简单，但对于一般损失函数而言，往往每一步优化都要不那么容易。所以出现了梯度提升树，梯度提升树利用最速下降法的近似方法，其关键是利用损失函数的负梯度在当前模型的值作为回归问题提升树算法中的残差的近似值，拟合一个回归树。

提升树和梯度提升树主要是在拟合损失上存在区别，一个是拟合残差（提升树）一个是使用负梯度（梯度提升树）作为损失，当GBDT使用最小二乘法作为损失函数时，GBDT 也变成拟合残差了。但是一般GBDT 不使用最小二乘法作为损失函数。
 


## `一、提升树`

### `1.1、提升树引出：`

* 1、提升树通过前向分布算法：

    <div align=center><img  src="./static/前向分布算法2.png"/></div>

* 2、可以看出我们的目标其实是优化损失函数

    <div align=center><img  src="./static/损失函数.png"/></div>

* 3、给定当前模型fm-1,需要求解：

    <div align=center><img  src="./static/最优参数θ.png"/></div>

    得到最优参数 θ，即 m 棵树的参数。

* 4、采用平方误差损失函数时：

    <div align=center><img  src="./static/均方误差损失.png"/></div>

* 5、简化成拟合残差

    <div align=center><img  src="./static/残差.png"/></div>


### `1.2、提升树 - 回归树案例：`

<div align=center><img  src="./static/提升回归树案例1.png"/></div>

`流程：`

* 1、设定分割点，按这个案例来说，会考虑如下分割点：

        1.5 2.5 3.5 4.5 5.5 6.5 7.5 8.5 9.5 

* 2、然后对数据进行分割，计算最小损失：

    `如当分割点为 1.5 时：`

        c1 = 5.56
        c2 = 7.50

        左节点损失：(5.56 - 5.56)² = 0

        右节点损失：(5.7 - 7.50)² + (5.91 - 7.50)² + ...... + (9.05 - 7.50)² = 15.72

        总损失 = 左节点损失 + 右节点损失 = 0 + 15.72 = 15.72

    `当分割点为 2.5 时: `


        按照如上方法计算，得到最后的总损失

        ......

    经过计算后发现以6.5为分割点得到的损失最小，那么 `回归树 T1` 为：

    <div align=center><img  src="./static/回归树T1_1.png"/></div>

* 3、接下来再进行学习的时候拟合的不再是原数据了，而是拟合残差：

    <div align=center><img  src="./static/残差表.png"/></div>

* 4、按照优化问题重新计算损失，选择最小的损失节点作为分割点,得到 `回归树T2` ：


    <div align=center><img  src="./static/回归树T2.png"/></div>

* 5、以此类推求得 T3、T4、T5、T6 .... 直到最后这些树足够拟合数据，残差达到你的满意值后停止训练。

* 6、如第六次停止训练后f6(x)为：

    <div align=center><img  src="./static/回归树f6.png"/></div>



## `二、GBDT 回归树`

和提升树差不多，只不过拟合的不是残差，而是负梯度。


残差只是负梯度的特例，最终的目的是使得损失函数减少的最快，沿哪个方向减少最快呢？负梯度方向。拟合负梯度，相当于损失函数沿着负梯度方向减少。当损失函数为平方损失时，拟合残差就相当于平方损失沿着负梯度方向减小。但是当损失函数不是平方损失时，再拟合残差就不是损失函数沿着负梯度减小了，所以此时当然不能再拟合残差。



## `三、GBDT 分类树`

这里我们再看看GBDT分类算法，GBDT的分类算法从思想上和GBDT的回归算法没有区别，但是由于样本输出不是连续的值，而是离散的类别，导致我们无法直接从输出类别去拟合类别输出的误差。

为了解决这个问题，主要有两个方法，`一个是用指数损失函数，此时GBDT退化为Adaboost算法`。另一种方法是用类似于逻辑回归的对数似然损失函数的方法。也就是说，我们用的是类别的预测概率值和真实概率值的差来拟合损失。本文仅讨论用对数似然损失函数的GBDT分类。而对于对数似然损失函数，我们又有二元分类和多元分类的区别。







## 参考链接

* 梯度提升树(GBDT)原理小结： https://www.cnblogs.com/pinard/p/6140514.html