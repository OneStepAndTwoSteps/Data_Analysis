
## Xgboost

__Xgboost是一种boosting集成算法(串行)，本身是由很多CART回归树集成。__

那什么是CART树？

__CART树分两种：__

    分类树：

        分类树分析是指预测结果是数据所属的类（比如某个电影去看还是不看） 分类树用于离散数据

    回归树：

        回归树分析是指预测结果可以被认为是实数（例如房屋的价格，或患者在医院中的逗留时间）回归树用于连续数据

我们知道 __分类树__ 的节点分裂可以依靠信息增益（ID3）、信息增益率（C4.5）、基尼系数（CART）等等方法进行分裂判断(比如我们在判断一个人是男是女，我们可以通过判断是否为长头发来分类男生女生，但是我们也可以通过是否穿高更鞋，是否有喉结这些特征进行判断，信息增益（ID3）、信息增益率（C4.5）、基尼系数（CART）等等方法可以帮助我们选择最优的结果作为我们的分裂点)。

CART 可以用于作为回归树划分数据集其过程和分类树的过程是一样的，只是回归树得到的预测结果是连续值， __而且评判“不纯度”的指标不同__,但是息增益（ID3）、信息增益率（C4.5）只支持分类模型。

对于 __回归树__ ，你没法再用分类树那套信息增益、信息增益率、基尼系数来判定树的节点分裂了， CART 分类树中采用的是基尼系数作为标准，那么在 CART 回归树中，如何评价“不纯度”呢？你需要采取新的方式评估效果，包括预测误差（常用的有均方误差、对数误差等），也就是样本的离散程度来评价“不纯度”。


### GBDT

说到Xgboost，不得不先从GBDT(Gradient Boosting Decision Tree)说起。因为 __xgboost本质上还是一个GBDT__，但是力争把速度和效率发挥到极致，所以叫X (Extreme) GBoosted。包括前面说过，两者都是boosting方法。

__GBDT的原理很简单，就是所有弱分类器的结果相加等于预测值__，然后 __下一个弱分类器去拟合误差函数对预测值的梯度(残差)__ 这个梯度(残差)就是预测值与真实值之间的误差。

__举个例子：__

假如现在我们想去银行贷款1000元，但计算机或者模型GBDT并不知道我想去银行贷款多少元，那GBDT咋办呢？

    1、它会在第一个弱分类器（或第一棵树中）随便用一个贷款金额比如950元来拟合，然后发现误差有50元；

    2、接下来在第二棵树中，用30元去拟合剩下的损失(第一个模型生成的残差1000-950=50元)，发现差距还有20元；

    3、接着在第三棵树中用15元拟合剩下的差距，发现差距只有5元了；

    4、最后在第四课树中用5元拟合剩下的残差.

__最终，四棵树的结论加起来__，就是银行贷款1000元。实际工程中，gbdt是计算负梯度，用负梯度近似残差。


### Xgboost

XGBoost是用于处理标准表格数据（存储在Pandas DataFrames中的数据类型，而不是图像和视频等更奇特的数据类型）的领先模型.


__举个例子:__

我们要预测一家人对电子游戏的喜好程度，考虑到年轻和年老相比，年轻更可能喜欢电子游戏，以及男性和女性相比，男性更喜欢电子游戏，故先根据年龄大小区分小孩和大人，然后再通过性别区分开是男是女，逐一给各人在电子游戏喜好程度上打分，如下图所示。

<div align=center><img  src="./static/1.png"/></div>

就这样，训练出了2棵树tree1和tree2，类似之前gbdt的原理，两棵树的结论累加起来便是最终的结论，所以小孩的预测分数就是两棵树中小孩所落到的结点的分数相加：2 + 0.9 = 2.9。爷爷的预测分数同理：-1 + （-0.9）= -1.9。


__现在我们可以看出xgboost与gbdt有着异曲同工之妙，一样也是通过将所有弱分类器的结果相加得到最终的预测值，但两者并不完全相同，两者的目标函数不一样。xgboost的目标函数如下图所示：__

<div align=center><img  src="./static/6.png"/></div>


    一、1为我们t-1个模型的残差，2为我们的新加的树模型，整个l()就是我们的损失函数。

    二、3为我们的正则项（包括L1正则、L2正则）

    三、constant 是我们的常数项

额，突然丢这么大一个公式，不少人可能瞬间就懵了。没事，下面咱们来拆解下这个目标函数，并一一剖析每个公式、每个符号、每个下标的含义。

#### Xgboost的核心思想

    1、不断地添加树，不断地进行特征分裂来生长一棵树，每次添加一个树，其实是学习一个新函数，去拟合上次预测的残差。

    2、当我们训练完成得到k棵树，我们要预测一个样本的分数，其实就是根据这个样本的特征，在每棵树中会落到对应的一个叶子节点，每个叶子节点就对应一个分数。

    3、最后只需要将每棵树对应的分数加起来就是该样本的预测值。


#### 模型学习与训练误差

具体来说，目标函数第一部分中的 i 表示第个样本， l(^y^i − y^i ) 表示第 i 个样本的预测误差，我们的目标当然是误差越小越好。

类似之前GBDT的套路，xgboost也是需要将多棵树的得分累加得到最终的预测得分（每一次迭代，都在现有树的基础上，增加一棵树去拟合前面树的预测结果与真实值之间的残差）。

__下图是xgboost的预测算法模型：__

<div align=center><img  src="./static/2.png"/></div>

__之前我们说过，我们会根据前面的树模型的残差作为下一个模型的训练值__

那么我们如何选择每一轮加入什么 f(树) 呢？ 答案是非常直接的，我们想要的是选取一个f(树) 可以使得我们的目标函数尽量最大地降低。__下图是目标函数。__

<div align=center><img  src="./static/5.png"/></div>


上面那个Obj的公式表达的可能有些过于抽象，我们可以考虑当是平方误差的情况（相当于 l(y^i,^y^i)=(^y^i-y^i)^2），这个时候我们的目标可以被写成下面这样的二次函数（图中画圈的部分表示的就是预测值和真实值之间的残差）：

<div align=center><img  src="./static/5-1.png"/></div>

__其实这里我们要做的就是如何加入一个新的模型，能使得我们的整个模型更加优化，误差越来越小。__

#### 正则项：树的复杂度

__首先，梳理下几个规则__

    用叶子节点集合以及叶子节点得分表示 

    每个样本都落在一个叶子节点上 
    
    q(x)表示样本x在某个叶子节点上，wq(x)是该节点的打分，即该样本的模型预测值



__我们也可以通过W_q(x)(叶子节点q的分数) 来代表一棵决策树模型，如下图所示：__

<div align=center><img  src="./static/3.png"/></div>



__但是有一点是，当我们树模型变多时，可能会给我们带来过拟合的效果，这里我们引入正则化项：__

<div align=center><img  src="./static/4.png"/></div>


T表示叶子节点的个数，w表示叶子节点的分数。直观上看，目标要求预测误差尽量小，且叶子节点T尽量少（γ控制叶子结点的个数），节点数值w尽量不极端（λ控制叶子节点的分数不会过大），防止过拟合。

    T表示的是树里面叶子节点的个数

    w的L2模平方是对w进行L2正则化，相当于针对每个叶结点的得分增加L2平滑，目的是为了避免过拟合

__最后我们可以得到目标函数：__

<div align=center><img  src="./static/8.png"/></div>

这里略过推导过程，具体可以参考下面的参考链接

### 分裂节点

之前我们说过回归树不在能使用分类树的分裂评估方法，所以接下去我们讨论如何进行回归树的分裂。

对于一个叶子节点如何进行分裂，xgboost作者在其原始论文中给出了两种分裂节点的方法：

__1）枚举所有不同树结构的贪心法__

现在的情况是只要知道树的结构，就能得到一个该结构下的最好分数，那如何确定树的结构呢？

一个想当然的方法是：不断地枚举不同树的结构，然后利用打分函数来寻找出一个最优结构的树，接着加入到模型中，不断重复这样的操作。而再一想，你会意识到要枚举的状态太多了，基本属于无穷种，那咋办呢？

我们试下贪心法，从树深度0开始，每一节点都遍历所有的特征，比如年龄、性别等等，然后对于某个特征，先按照该特征里的值进行排序，然后线性扫描该特征进而确定最好的分割点，最后对所有特征进行分割后，我们选择所谓的增益Gain最高的那个特征，而Gain如何计算呢？

__目标函数中的G/(H+λ)部分，表示着每一个叶子节点对当前模型损失的贡献程度，融合一下，得到Gain的计算表达式，如下所示：__

<div align=center><img  src="./static/7.png"/></div>


__第一个值得注意的地方__ 是“对于某个特征，先按照该特征里的值进行排序”，这里举个例子。

比如设置一个值a，然后枚举所有x < a、a  < x这样的条件（x代表某个特征比如年龄age，把age从小到大排序：假定从左至右依次增大，则比a小的放在左边，比a大的放在右边），对于某个特定的分割a，我们要计算a左边和右边的导数和。

比如总共五个人，按年龄排好序后，一开始我们总共有如下4种划分方法：

    把第一个人和后面四个人划分开

    把前两个人和后面三个人划分开

    把前三个人和后面两个人划分开

    把前面四个人和后面一个人划分开

__接下来，把上面4种划分方法全都各自计算一下Gain，看哪种划分方法得到的Gain值最大则选取哪种划分方法__，经过计算，发现把第2种划分方法“前面两个人和后面三个人划分开”得到的Gain值最大，意味着在一分为二这个第一层层面上这种划分方法是最合适的,那么我们就选择这个特征作为分裂的节点。

换句话说，对于所有的特征x，我们只要做一遍从左到右的扫描就可以枚举出所有分割的梯度和GL和GR。然后用计算Gain的公式计算每个分割方案的分数就可以了。

然后后续则依然按照这种划分方法继续第二层、第三层、第四层、第N层的分裂。


__第二个值得注意的地方__ 是引入分割不一定会使得情况变好，所以我们有一个引入新叶子的惩罚项。优化这个目标对应了树的剪枝， 当引入的分割带来的增益小于一个阀值γ 的时候，则忽略这个分割。

换句话说，当引入某项分割，结果分割之后得到的分数 -(减去) 不分割得到的分数得到的值太小（比如小于我们的最低期望阀值γ），但却因此得到的复杂度过高，则相当于得不偿失，不如不分割。即做某个动作带来的好处比因此带来的坏处大不了太多，则为避免复杂 多一事不如少一事的态度，不如不做。

相当于在我们发现“分”还不如“不分”的情况下后（得到的增益太小，小到小于阈值γ），会有2个叶子节点存在同一棵子树上的情况，就像下图这样。

<div align=center><img  src="./static/9.png"/></div>


__2）近似算法__

主要针对数据太大，不能直接进行计算

#### 为了防止一直分裂，限制树生长过深，我们可以通过以下方法来进行限制：

1、为了限制树生长过深，我们可以设置阈值，只有当增益大于该阈值才进行分裂。当引入的分裂带来的增益小于设定阀值的时候，我们可以忽略掉这个分裂，所以并不是每一次分裂loss function整体都会增加的，有点预剪枝的意思，阈值参数为（即正则项里叶子节点数T的系数）； 

2.当树达到最大深度时则停止建立决策树，设置一个超参数max_depth，避免树太深导致学习局部样本，从而过拟合； 

3、当样本权重和小于设定阈值时则停止建树。什么意思呢，即涉及到一个超参数-最小的样本权重和min_child_weight，和GBM的 min_child_leaf 参数类似，但不完全一样。大意就是一个叶子节点样本太少了，也终止同样是防止过拟合； 



-《[笔记参考来源](https://blog.csdn.net/v_july_v/article/details/81410574)》
