## 集成算法模型：

### Bagging 和 Pasting：

 __并行。__ 训练多个分类器取平均,如随机森林中我们并行运行多个随机数，如果是回归任务我们可以取决策树们的平均值，如果是分类问题我们可以取分类结果中的众数(比如分类结果 0 和 1 ，如果决策出的结果0较多，那么最终分类结果取0) 
 
*   Bagging： 有放回采样被 称为装袋（Bagging，是 bootstrap aggregating 的缩写）

*   pasting： 无放回采样称为粘贴 （pasting）

__典型的算法就是随机森林__，可以帮助我们提高泛化能力。


### Boosting:

 __串行。__ 从弱学习器开始加强，通过加权来进行训练，__常见的Boosting模型算法有Adaboost、gbrt,Xgboost算法__。

*   Adaboost会根据预测的准确性来进行进行权重调整

*   gbrt 就是使用残差来训练模型。
    
    举个例子(gbrt)：当我们去银行贷款1000元，但是第一颗树模型(A)预测结果为贷款950元，但是我们的真实值是贷款了100，我们的误差为1000-950=50，接下来我们就需要减少这个误差(50),在我们第二棵树模型(B)就需要在这个50上再进行预测，假设预测结果为30，假设我们就只有三颗树，那么第三颗树(C) 就不再看第B颗树的结果了，而是结合A和B的结果(1000-950-30=20),C树预测的残差值就是20，这个时候如果C数预测的是18，__这个时候根据下面的公式，意思就是我们的之前模型(红色框)+我们现在的模型(紫色框)__ 我们将我们的所有树的结果相加我们的预测值就是998，这个时候我们的效果就很好了。按这个例子来讲的话boosting就是算出A树然后在A树的基础上计算出残差，然后再构造B树，然后C树。串行。


### Stacking:

__堆叠算法__，可以堆叠各种各样的分类器（KNN,SVM,RF等等），堆叠算法分阶段：第一阶段得出各自结果，第二阶段再用前一阶段结果训练。

举个例子：

*   __第一阶段：__ 现在我们分别构建了RF、LR、KNN、SVM算法分类模型，分类器之间的分类结果可能不同，假如我们现在的分类是二分类，现在分类第一个样本，可能 RF分类的结果为1、LR分类的结果为1、KNN分类的结果为1、SVM分类的结果为0，分类第二个样本，可能 RF分类的结果为0、LR分类的结果为1、KNN分类的结果为1、SVM分类的结果为0，分类第三个样本，可能 RF分类的结果为0、LR分类的结果为0、KNN分类的结果为0、SVM分类的结果为0。

*   __第二个阶段：__ 第二个阶段会将第一个阶段的分类结果输入到一个模型(如LR模型)中，然后输出最终的分类结果。

__简而言之：__ 第一个阶段：我们制造多个模型(如分类器)，然后进行特征的输入，得到各个模型的预测结果。第二个阶段，我们将第一个阶段的所有预测结果作为我们这个阶段的输入，输入到我们的一个模型中，最终输出我们的最终结果。


