## EM聚类的工作原理

EM算法也称期望最大化（Expectation-Maximum,简称EM）算法它是一个基础算法，是很多机器学习领域算法的基础，比如隐式马尔科夫算法（HMM）， LDA主题模型的变分推断等等。

EM算法解决这个的思路是使用启发式的迭代方法，既然我们无法直接求出模型分布参数，那么我们可以先猜想隐含数据（EM算法的E步），接着基于观察数据和猜测的隐含数据一起来极大化对数似然，求解我们的模型参数（EM算法的M步)。由于我们之前的隐藏数据是猜测的，所以此时得到的模型参数一般还不是我们想要的结果。不过没关系，我们基于当前得到的模型参数，继续猜测隐含数据（EM算法的E步），然后继续极大化对数似然，求解我们的模型参数（EM算法的M步)。以此类推，不断的迭代下去，直到模型分布参数基本无变化，算法收敛，找到合适的模型参数。

### 最大似然估计
最大似然估计是什么呢？它指的就是一件事情已经发生了，然后反推更有可能是什么因素造成的。用一男一女比较身高为例，假设有一个人比另一个人高，反推他可能是男性(因为相同年龄下男性的平均身高高于女性)。最大似然估计是一种通过已知结果，估计参数的方法。

__EM 算法是一种求解最大似然估计的方法，通过观测样本，来找出样本的模型参数。__

### 举个例子来描述EM的工作原理
举了例子：我们先看一个简单的场景：假设你炒了一份菜，如何平均分到两个盘子中？

正常来说我们不会使用称来称重，我们通常会先随机分成两份，然后观察两份的盘子中的菜是不是一样多，哪个盘子多就匀一点到少的盘子中，然后再观察，直到两个碟子中的菜一样。

我们能从这个例子中看到三个主要的步骤：初始化参数、观察预期、重新估计。

首先是先给每个碟子初始化一些菜量，然后再观察预期，这两个步骤实际上就是 __期望步骤（Expectation）__ 。如果结果存在偏差就需要重新估计参数，这个就是 __最大化步骤（Maximization）。__ 这两个步骤加起来也就是 EM 算法的过程。

我们实际上就是想要让两个盘子中的菜是一样多的，我们可以把盘子中菜的份量理解成我们相求的模型参数，我们通过EM算法中的E(Expectation)步来进行观察，然后通过M(Maximization)步来进行两个盘子中的菜的份量，最后让两个盘子中的份量不再发生变化为止。

### 再举一个使用EM思想的例子：

举个一个投掷硬币的例子，假设我们有 A 和 B 两枚硬币，我们做了 5 组实验，每组实验投掷 10 次，然后统计出现正面的次数，实验结果如下：

![Image_text](https://raw.githubusercontent.com/OneStepAndTwoSteps/data_mining_analysis/master/static/EM%E8%81%9A%E7%B1%BB/1.png)

投掷硬币这个过程中存在隐含的数据，即我们事先并不知道每次投掷的硬币是 A 还是 B。假设我们知道这个隐含的数据，并将它完善，可以得到下面的结果：

![Image_text](https://raw.githubusercontent.com/OneStepAndTwoSteps/data_mining_analysis/master/static/EM%E8%81%9A%E7%B1%BB/2.png)

此时我们想要求得硬币 A 和 B 出现正面次数的概率，可以直接求得：

    θA=(5+4)/(10+10)=0.45
    θB=(7+8+9)/(10+10+10)=0.8
    
而实际情况是我不知道每次投掷的硬币是 A 还是 B，那么如何求得硬币 A 和硬币 B 出现正面的概率呢？    

这里就需要采用 EM 算法的思想。   

__1. 初始化参数。__

我们假设硬币 A 和 B 的正面概率（随机指定）是θA=0.5 和θB=0.9。

__2. 计算期望值。__

假设实验 1 投掷的是硬币 A，那么正面次数为 5 的概率为： 

<div align=center><img  src="https://raw.githubusercontent.com/OneStepAndTwoSteps/data_mining_analysis/master/static/EM%E8%81%9A%E7%B1%BB/3.png"/></div>

公式中的 C(10,5) 代表的是 10 个里面取 5 个的组合方式，也就是排列组合公式，0.5 的 5 次方乘以 0.5 的 5 次方代表的是其中一次为 5 次为正面，5 次为反面的概率，然后再乘以 C(10,5) 等于正面次数为 5 的概率。       

假设实验 1 是投掷的硬币 B ，那么正面次数为 5 的概率为：
<div align=center><img src="https://raw.githubusercontent.com/OneStepAndTwoSteps/data_mining_analysis/master/static/EM%E8%81%9A%E7%B1%BB/4.png"/></div>
   

所以实验 1 更有可能投掷的是硬币 A。   
   
__3. 通过猜测的结果{A, A, B, B, A}来完善初始化的参数θA 和θB。__

我们可以看出 EM 算法中的 E 步骤就是通过旧的参数来计算隐藏变量。然后在 M 步骤中，通过得到的隐藏变量的结果来重新估计参数。直到参数不再发生变化，得到我们想要的结果。
   
   
### 把EM理解成框架，这个框架中可以使用不同的模型来用于EM进行求解。

__常见的EM聚类：__

    GMM 高斯混合模型和 HMM 隐马尔科夫模型。GMM（高斯混合模型）聚类就是 EM 聚类的一种。
    
GMM 是通过概率密度来进行聚类，聚成的类符合高斯分布（正态分布）。而 HMM 用到了马尔可夫过程，在这个过程中，我们通过状态转移矩阵来计算状态转移的概率。HMM 在自然语言处理和语音识别领域中有广泛的应用。
   
### 在sklearn中初始化聚类模型：

    from sklearn.mixture import GaussianMixture

    #创建GMM(高斯混合模型)聚类
    gmm== GaussianMixture(n_components=1, covariance_type=‘full’, max_iter=100) \
    
__参数讲解:__

    1.n_components：即高斯混合模型的个数，也就是我们要聚类的个数，默认值为 1。如果不指定 n_components，最终的聚类结果都会为同一个值。
    2.covariance_type：代表协方差类型。一个高斯混合模型的分布是由均值向量和协方差矩阵决定的，所以协方差的类型也代表了不同的高斯混合模型的特征。
    协方差类型有 4 种取值：
    
        covariance_type=full，代表完全协方差，也就是元素都不为 0；
        covariance_type=tied，代表相同的完全协方差；
        covariance_type=diag，代表对角协方差，也就是对角不为 0，其余为 0；
        covariance_type=spherical，代表球面协方差，非对角为 0，对角完全相同，呈现球面的特性。

    3.max_iter：代表最大迭代次数，EM 算法是由 E 步和 M 步迭代求得最终的模型参数，这里可以指定最大迭代次数，默认值为 100。



   
