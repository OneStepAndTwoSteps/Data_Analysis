{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **Homework 6 - Generative Adversarial Network**\n\nThis is the example code of homework 6 of the machine learning course by Prof. Hung-yi Lee.\n\n\nIn this homework, you are required to build a generative adversarial  network for anime face generation.\n","metadata":{"id":"oZ-C2Dgetg37"}},{"cell_type":"markdown","source":"## Set up the environment\n","metadata":{"id":"JTBkY5QFf3QM"}},{"cell_type":"markdown","source":"### Packages Installation","metadata":{"id":"Y7y4wyYdEABR"}},{"cell_type":"code","source":"# You may replace the workspace directory if you want.\nworkspace_dir = '.'\n\n# Training progress bar\n! pip install -q qqdm\n!pip install gdown","metadata":{"id":"6IQB485dD_eL","execution":{"iopub.status.busy":"2022-08-18T07:55:45.324865Z","iopub.execute_input":"2022-08-18T07:55:45.325301Z","iopub.status.idle":"2022-08-18T07:56:22.271847Z","shell.execute_reply.started":"2022-08-18T07:55:45.325216Z","shell.execute_reply":"2022-08-18T07:56:22.270584Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"### Download the dataset\n**Please use the link according to the last digit of your student ID first!**\n\nIf all download links fail, please follow [here](https://drive.google.com/drive/folders/13T0Pa_WGgQxNkqZk781qhc5T9-zfh19e).\n\n* To open the file using your browser, use the link below (replace the id first!):\nhttps://drive.google.com/file/d/REPLACE_WITH_ID\n* e.g. https://drive.google.com/file/d/1IGrTr308mGAaCKotpkkm8wTKlWs9Jq-p","metadata":{"id":"2NFjuZTPDxLn"}},{"cell_type":"code","source":"!gdown --id 1IGrTr308mGAaCKotpkkm8wTKlWs9Jq-p --output \"{workspace_dir}/crypko_data.zip\"\n\n# Other download links\n#   Please uncomment the line according to the last digit of your student ID first\n\n# 0\n# !gdown --id 131zPaVoi-U--XThvzgRfaxrumc3YSBd3 --output \"{workspace_dir}/crypko_data.zip\"\n\n# 1\n# !gdown --id 1kCuIj1Pf3T2O94H9bUBxjPBKb---WOmH --output \"{workspace_dir}/crypko_data.zip\"\n\n# 2\n# !gdown --id 1boEoiiqBJwoHVvjmI0xgoutE9G0Rv8CD --output \"{workspace_dir}/crypko_data.zip\"\n\n# 3\n# !gdown --id 1Ic0mktAQQvnNAnswrPHsg-u2OWGBXTWF --output \"{workspace_dir}/crypko_data.zip\"\n\n# 4\n# !gdown --id 1PFcc25r9tLE7OyQ-CDadtysNdWizk6Yg --output \"{workspace_dir}/crypko_data.zip\"\n\n# 5\n# !gdown --id 1wgkrYkTrhwDSMdWa5NwpXeE4-7JaUuX2 --output \"{workspace_dir}/crypko_data.zip\"\n\n# 6\n# !gdown --id 19gwNYWi9gN9xVL86jC3v8qqNtrXyq5Bf --output \"{workspace_dir}/crypko_data.zip\"\n\n# 7 \n# !gdown --id 1-KPZB6frRSRLRAtQfafKCVA7em0_NrJG --output \"{workspace_dir}/crypko_data.zip\"\n\n# 8\n# !gdown --id 1rNBfmn0YBzXuG5ub7CXbsGwduZqEs8hx --output \"{workspace_dir}/crypko_data.zip\"\n\n# 9\n# !gdown --id 113NEISX-2j6rBd1yyBx0c3_9nPIzSNz- --output \"{workspace_dir}/crypko_data.zip\"\n","metadata":{"id":"uZomvVA2f607","outputId":"df63fbec-05c4-4e21-9fe7-6d7f3c54b095","execution":{"iopub.status.busy":"2022-08-18T07:56:22.275124Z","iopub.execute_input":"2022-08-18T07:56:22.275843Z","iopub.status.idle":"2022-08-18T07:56:30.347821Z","shell.execute_reply.started":"2022-08-18T07:56:22.275798Z","shell.execute_reply":"2022-08-18T07:56:30.346650Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"###Unzip the downloaded file.\nThe unzipped tree structure is like \n```\nfaces/\n├── 1.jpg\n├── 2.jpg\n├── 3.jpg\n...\n```","metadata":{"id":"pNtT1WCOyRNt"}},{"cell_type":"code","source":"!unzip -q \"{workspace_dir}/crypko_data.zip\" -d \"{workspace_dir}/\"","metadata":{"id":"s2qR-0hjqWE6","outputId":"a525e069-6929-450d-8da2-24c465cc5774","execution":{"iopub.status.busy":"2022-08-18T07:56:31.162924Z","iopub.execute_input":"2022-08-18T07:56:31.163297Z","iopub.status.idle":"2022-08-18T07:56:40.350647Z","shell.execute_reply.started":"2022-08-18T07:56:31.163263Z","shell.execute_reply":"2022-08-18T07:56:40.348650Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"## Random seed\nSet the random seed to a certain value for reproducibility.","metadata":{"id":"NjfM46dtmxXj"}},{"cell_type":"code","source":"import random\n\nimport torch\nimport numpy as np\n\n\ndef same_seeds(seed):\n    # Python built-in random module\n    random.seed(seed)\n    # Numpy\n    np.random.seed(seed)\n    # Torch\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed(seed)\n        torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.benchmark = False\n    torch.backends.cudnn.deterministic = True\n\nsame_seeds(2021)","metadata":{"id":"OWuecW1imz42","execution":{"iopub.status.busy":"2022-08-18T07:56:43.260243Z","iopub.execute_input":"2022-08-18T07:56:43.260987Z","iopub.status.idle":"2022-08-18T07:56:45.107643Z","shell.execute_reply.started":"2022-08-18T07:56:43.260951Z","shell.execute_reply":"2022-08-18T07:56:45.103794Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"## Import Packages\nFirst, we need to import packages that will be used later.\n\nLike hw3, we highly rely on **torchvision**, a library of PyTorch.","metadata":{"id":"uCTPz2iRQmwe"}},{"cell_type":"code","source":"import os\nimport glob\n\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torchvision\nimport torchvision.transforms as transforms\nfrom torch import optim\nfrom torch.autograd import Variable\nfrom torch.utils.data import Dataset, DataLoader\nimport matplotlib.pyplot as plt\nfrom qqdm.notebook import qqdm","metadata":{"id":"TC8RRsX0QhL-","execution":{"iopub.status.busy":"2022-08-18T07:56:47.371841Z","iopub.execute_input":"2022-08-18T07:56:47.372749Z","iopub.status.idle":"2022-08-18T07:56:47.865078Z","shell.execute_reply.started":"2022-08-18T07:56:47.372695Z","shell.execute_reply":"2022-08-18T07:56:47.862197Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"## Dataset\n1. Resize the images to (64, 64)\n1. Linearly map the values from [0, 1] to  [-1, 1].\n\nPlease refer to [PyTorch official website](https://pytorch.org/vision/stable/transforms.html) for details about different transforms.\n","metadata":{"id":"kYjZ_G83_YX4"}},{"cell_type":"code","source":"class CrypkoDataset(Dataset):\n    def __init__(self, fnames, transform):\n        self.transform = transform\n        self.fnames = fnames\n        self.num_samples = len(self.fnames)\n\n    def __getitem__(self,idx):\n        fname = self.fnames[idx]\n        # 1. Load the image\n        img = torchvision.io.read_image(fname)\n        # 2. Resize and normalize the images using torchvision.\n        img = self.transform(img)\n        return img\n\n    def __len__(self):\n        return self.num_samples\n\n\ndef get_dataset(root):\n   \n    fnames = glob.glob(os.path.join(root, '*'))   ## glob.glob(os.path.join(os.path.join(workspace_dir, 'faces'), '*'))[:10]  ## 获取目录下的文件名\n    # 1. Resize the image to (64, 64)\n    # 2. Linearly map [0, 1] to [-1, 1]\n    compose = [\n        transforms.ToPILImage(),\n        transforms.Resize((64, 64)),\n        transforms.ToTensor(),\n        transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5)),\n    ]\n    transform = transforms.Compose(compose)\n    dataset = CrypkoDataset(fnames, transform)\n    return dataset","metadata":{"id":"UZ6d0_cr8R26","execution":{"iopub.status.busy":"2022-08-18T11:17:18.564063Z","iopub.execute_input":"2022-08-18T11:17:18.564463Z","iopub.status.idle":"2022-08-18T11:17:18.573429Z","shell.execute_reply.started":"2022-08-18T11:17:18.564429Z","shell.execute_reply":"2022-08-18T11:17:18.572029Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"markdown","source":"### Show some images\nNote that the values are in the range of [-1, 1], we should shift them to the valid range, [0, 1], to display correctly.","metadata":{"id":"tGwdVhOKSjLY"}},{"cell_type":"code","source":"glob.glob(os.path.join(os.path.join(workspace_dir, 'faces'), '*'))[:10]","metadata":{"execution":{"iopub.status.busy":"2022-08-18T07:56:51.591885Z","iopub.execute_input":"2022-08-18T07:56:51.592285Z","iopub.status.idle":"2022-08-18T07:56:51.906529Z","shell.execute_reply.started":"2022-08-18T07:56:51.592250Z","shell.execute_reply":"2022-08-18T07:56:51.905385Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"dataset = get_dataset(os.path.join(workspace_dir, 'faces'))\n\nimages = [dataset[i] for i in range(16)]\ngrid_img = torchvision.utils.make_grid(images, nrow=4)\nplt.figure(figsize=(10,10))\nplt.imshow(grid_img.permute(1, 2, 0))\nplt.show()","metadata":{"id":"34mVNtHn7cwF","outputId":"5a657b52-ce4d-4f5a-e5a8-d15d0408be5f","execution":{"iopub.status.busy":"2022-08-18T07:57:23.418953Z","iopub.execute_input":"2022-08-18T07:57:23.419614Z","iopub.status.idle":"2022-08-18T07:57:24.051576Z","shell.execute_reply.started":"2022-08-18T07:57:23.419574Z","shell.execute_reply":"2022-08-18T07:57:24.050605Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"images[1]","metadata":{"execution":{"iopub.status.busy":"2022-08-18T07:49:13.329937Z","iopub.execute_input":"2022-08-18T07:49:13.330375Z","iopub.status.idle":"2022-08-18T07:49:13.341918Z","shell.execute_reply.started":"2022-08-18T07:49:13.330335Z","shell.execute_reply":"2022-08-18T07:49:13.340525Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"images = [(dataset[i]+1)/2 for i in range(16)]\ngrid_img = torchvision.utils.make_grid(images, nrow=4)\nplt.figure(figsize=(10,10))\nplt.imshow(grid_img.permute(1, 2, 0))\nplt.show()","metadata":{"id":"nhxUjRUuHdti","outputId":"b8277a3b-fe40-485c-c004-1652dba27dd3","execution":{"iopub.status.busy":"2022-08-18T07:57:27.564418Z","iopub.execute_input":"2022-08-18T07:57:27.565059Z","iopub.status.idle":"2022-08-18T07:57:28.024006Z","shell.execute_reply.started":"2022-08-18T07:57:27.565023Z","shell.execute_reply":"2022-08-18T07:57:28.023201Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"images[1]","metadata":{"execution":{"iopub.status.busy":"2022-08-18T07:49:15.394455Z","iopub.execute_input":"2022-08-18T07:49:15.394939Z","iopub.status.idle":"2022-08-18T07:49:15.407871Z","shell.execute_reply.started":"2022-08-18T07:49:15.394898Z","shell.execute_reply":"2022-08-18T07:49:15.406415Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"markdown","source":"## Model\n\n* **DCGAN:**\n\n    Here, we use DCGAN as the model structure. Feel free to modify your own model structure.\n\n    Note that the `N` of the input/output shape stands for the batch size.\n\n\n* **WGAN:**\n    \n    判别器最后一层去掉sigmoid\n\n    生成器和判别器的loss不再取log\n\n    weight clipping: 对参数w做一个限制，如果发现w大于某个常数c，就令w = c; 如果w<-c , w = -c，等于说将参数限制在某一个范围内。(目的是为了限制discriminator不要变化得太剧烈，尽量平滑）\n\n    不要用基于动量的优化算法（momentum、Adam等），是一个tricks","metadata":{"id":"XkvZ4JgCHCZD"}},{"cell_type":"code","source":"def weights_init(m):\n    classname = m.__class__.__name__\n    if classname.find('Conv') != -1:\n        m.weight.data.normal_(0.0, 0.02)\n    elif classname.find('BatchNorm') != -1:\n        m.weight.data.normal_(1.0, 0.02)\n        m.bias.data.fill_(0)\n\nclass Generator(nn.Module):\n    \"\"\"\n    Input shape: (N, in_dim)\n    Output shape: (N, 3, 64, 64)\n    \"\"\"\n    def __init__(self, in_dim, dim=64):\n        super(Generator, self).__init__()\n        def dconv_bn_relu(in_dim, out_dim):\n            return nn.Sequential(\n                nn.ConvTranspose2d(in_dim, out_dim, 5, 2,\n                                   padding=2, output_padding=1, bias=False),\n                nn.BatchNorm2d(out_dim),\n                nn.ReLU()\n            )\n        self.l1 = nn.Sequential(\n            nn.Linear(in_dim, dim * 8 * 4 * 4, bias=False),\n            nn.BatchNorm1d(dim * 8 * 4 * 4),\n            nn.ReLU()\n        )\n        self.l2_5 = nn.Sequential(\n            dconv_bn_relu(dim * 8, dim * 4),\n            dconv_bn_relu(dim * 4, dim * 2),\n            dconv_bn_relu(dim * 2, dim),\n            nn.ConvTranspose2d(dim, 3, 5, 2, padding=2, output_padding=1),\n            nn.Tanh()\n        )\n        self.apply(weights_init)\n\n    def forward(self, x):\n        y = self.l1(x)\n        y = y.view(y.size(0), -1, 4, 4)\n        y = self.l2_5(y)\n        return y\n\n\nclass Discriminator(nn.Module):\n    \"\"\"\n    Input shape: (N, 3, 64, 64)\n    Output shape: (N, )\n    \"\"\"\n    def __init__(self, in_dim, dim=64):\n        super(Discriminator, self).__init__()\n\n        def conv_bn_lrelu(in_dim, out_dim):\n            return nn.Sequential(\n                nn.Conv2d(in_dim, out_dim, 5, 2, 2),\n                nn.BatchNorm2d(out_dim),\n                nn.LeakyReLU(0.2),\n            )\n            \n        \"\"\" Medium: Remove the last sigmoid layer for WGAN. \"\"\"\n        self.ls = nn.Sequential(\n            nn.Conv2d(in_dim, dim, 5, 2, 2), \n            nn.LeakyReLU(0.2),\n            conv_bn_lrelu(dim, dim * 2),\n            conv_bn_lrelu(dim * 2, dim * 4),\n            conv_bn_lrelu(dim * 4, dim * 8),\n            nn.Conv2d(dim * 8, 1, 4),\n#             nn.Sigmoid(), \n        )\n        self.apply(weights_init)\n        \n    def forward(self, x):\n        y = self.ls(x)\n        y = y.view(-1)\n        return y","metadata":{"id":"F0I1jRd6HFmm","execution":{"iopub.status.busy":"2022-08-18T07:57:41.963902Z","iopub.execute_input":"2022-08-18T07:57:41.964265Z","iopub.status.idle":"2022-08-18T07:57:41.980513Z","shell.execute_reply.started":"2022-08-18T07:57:41.964235Z","shell.execute_reply":"2022-08-18T07:57:41.979379Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"## Training","metadata":{"id":"Cxo4teqaO5RJ"}},{"cell_type":"markdown","source":"### Initialization\n- hyperparameters\n- model\n- optimizer\n- dataloader","metadata":{"id":"v5sCGIUtSViC"}},{"cell_type":"code","source":"# Training hyperparameters\nbatch_size = 64\nz_dim = 100\nz_sample = Variable(torch.randn(100, z_dim)).cuda()\nlr = 1e-4\n\n\"\"\" Medium: WGAN, 50 epoch, n_critic=5, clip_value=0.01 \"\"\"\n# n_epoch = 1 # 50   ## DC\n# n_critic = 1 # 5   ## DC \n\nn_epoch =  50\nn_critic =  5\nclip_value = 0.01\n\nlog_dir = os.path.join(workspace_dir, 'logs')\nckpt_dir = os.path.join(workspace_dir, 'checkpoints')\nos.makedirs(log_dir, exist_ok=True)\nos.makedirs(ckpt_dir, exist_ok=True)\n\n# Model\nG = Generator(in_dim=z_dim).cuda()\nD = Discriminator(3).cuda()\nG.train()\nD.train()\n\n# Loss\ncriterion = nn.BCELoss()\n\n\"\"\" Medium: Use RMSprop for WGAN. \"\"\"\n# Optimizer\n# opt_D = torch.optim.Adam(D.parameters(), lr=lr, betas=(0.5, 0.999))  ## DC\n# opt_G = torch.optim.Adam(G.parameters(), lr=lr, betas=(0.5, 0.999))  ## DC\nopt_D = torch.optim.RMSprop(D.parameters(), lr=lr)\nopt_G = torch.optim.RMSprop(G.parameters(), lr=lr)\n\n\n# DataLoader\ndataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n","metadata":{"id":"2EqomOouHezf","execution":{"iopub.status.busy":"2022-08-18T08:18:23.439055Z","iopub.execute_input":"2022-08-18T08:18:23.439471Z","iopub.status.idle":"2022-08-18T08:18:23.719683Z","shell.execute_reply.started":"2022-08-18T08:18:23.439437Z","shell.execute_reply":"2022-08-18T08:18:23.718674Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":"### Training loop\nWe store some pictures regularly to monitor the current \bperformance of the Generator, and regularly record checkpoints.","metadata":{"id":"vpJA1wzi0tii"}},{"cell_type":"code","source":"steps = 0\nfor e, epoch in enumerate(range(n_epoch)):\n    progress_bar = qqdm(dataloader)\n    for i, data in enumerate(progress_bar):\n        imgs = data\n        imgs = imgs.cuda()\n\n        bs = imgs.size(0)\n\n        # ============================================\n        #  Train D\n        # ============================================\n        z = Variable(torch.randn(bs, z_dim)).cuda()\n        r_imgs = Variable(imgs).cuda()\n        f_imgs = G(z)\n\n        \"\"\" Medium: Use WGAN Loss. \"\"\"\n        # Label\n        r_label = torch.ones((bs)).cuda()\n        f_label = torch.zeros((bs)).cuda()\n\n        # Model forwarding\n        r_logit = D(r_imgs.detach())\n        f_logit = D(f_imgs.detach())\n        \n        # Compute the loss for the discriminator.\n        r_loss = criterion(r_logit, r_label)\n        f_loss = criterion(f_logit, f_label)\n        loss_D = (r_loss + f_loss) / 2\n\n        # WGAN Loss\n        loss_D = -torch.mean(D(r_imgs)) + torch.mean(D(f_imgs))\n       \n\n        # Model backwarding\n        D.zero_grad()\n        loss_D.backward()\n\n        # Update the discriminator.\n        opt_D.step()\n\n        \"\"\" Medium: Clip weights of discriminator. \"\"\"\n        for p in D.parameters():\n            p.data.clamp_(-clip_value, clip_value)\n\n        # ============================================\n        #  Train G\n        # ============================================\n        if steps % n_critic == 0:\n            # Generate some fake images.\n            z = Variable(torch.randn(bs, z_dim)).cuda()\n            f_imgs = G(z)\n\n            # Model forwarding\n            f_logit = D(f_imgs)\n            \n            \"\"\" Medium: Use WGAN Loss\"\"\"\n            # Compute the loss for the generator.\n            loss_G = criterion(f_logit, r_label)\n            # WGAN Loss\n            loss_G = -torch.mean(D(f_imgs))\n\n            # Model backwarding\n            G.zero_grad()\n            loss_G.backward()\n\n            # Update the generator.\n            opt_G.step()\n\n        steps += 1\n        \n        # Set the info of the progress bar\n        #   Note that the value of the GAN loss is not directly related to\n        #   the quality of the generated images.\n        progress_bar.set_infos({\n            'Loss_D': round(loss_D.item(), 4),\n            'Loss_G': round(loss_G.item(), 4),\n            'Epoch': e+1,\n            'Step': steps,\n        })\n\n    G.eval()\n    f_imgs_sample = (G(z_sample).data + 1) / 2.0\n    filename = os.path.join(log_dir, f'Epoch_{epoch+1:03d}.jpg')\n    torchvision.utils.save_image(f_imgs_sample, filename, nrow=10)\n    print(f' | Save some samples to {filename}.')\n    \n    # Show generated images in the jupyter notebook.\n    grid_img = torchvision.utils.make_grid(f_imgs_sample.cpu(), nrow=10)\n    plt.figure(figsize=(10,10))\n    plt.imshow(grid_img.permute(1, 2, 0))\n    plt.show()\n    G.train()\n\n    if (e+1) % 5 == 0 or e == 0:\n        # Save the checkpoints.\n        torch.save(G.state_dict(), os.path.join(ckpt_dir, 'G.pth'))\n        torch.save(D.state_dict(), os.path.join(ckpt_dir, 'D.pth'))","metadata":{"id":"dgkqPih1o5Az","outputId":"29a89ab4-4e6a-4ebf-e845-0d5ee3b1a424","execution":{"iopub.status.busy":"2022-08-18T08:19:05.173924Z","iopub.execute_input":"2022-08-18T08:19:05.174368Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import PIL","metadata":{"execution":{"iopub.status.busy":"2022-08-18T11:26:11.325870Z","iopub.execute_input":"2022-08-18T11:26:11.326371Z","iopub.status.idle":"2022-08-18T11:26:11.333048Z","shell.execute_reply.started":"2022-08-18T11:26:11.326282Z","shell.execute_reply":"2022-08-18T11:26:11.332124Z"},"trusted":true},"execution_count":44,"outputs":[]},{"cell_type":"code","source":"path_to_image = './logs/Epoch_050.jpg'\nimage = PIL.Image.open(path_to_image)","metadata":{"execution":{"iopub.status.busy":"2022-08-18T11:26:52.396277Z","iopub.execute_input":"2022-08-18T11:26:52.396892Z","iopub.status.idle":"2022-08-18T11:26:52.402910Z","shell.execute_reply.started":"2022-08-18T11:26:52.396858Z","shell.execute_reply":"2022-08-18T11:26:52.401886Z"},"trusted":true},"execution_count":48,"outputs":[]},{"cell_type":"code","source":"imarray = np.asarray(image)   \nplt.figure(figsize=(12,12))\nplt.imshow(imarray)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-08-18T11:26:53.447267Z","iopub.execute_input":"2022-08-18T11:26:53.447835Z","iopub.status.idle":"2022-08-18T11:26:53.984008Z","shell.execute_reply.started":"2022-08-18T11:26:53.447797Z","shell.execute_reply":"2022-08-18T11:26:53.983213Z"},"trusted":true},"execution_count":49,"outputs":[]},{"cell_type":"markdown","source":"## Inference\nUse the trained model to generate anime faces!","metadata":{"id":"e2uJFmTtKBeH"}},{"cell_type":"markdown","source":"### Load model ","metadata":{"id":"tXPXcVD_HJB2"}},{"cell_type":"code","source":"import torch\n\nG = Generator(z_dim)\nG.load_state_dict(torch.load(os.path.join(ckpt_dir, 'G.pth')))\nG.eval()\nG.cuda()","metadata":{"id":"4JnQdNx2SUS2","outputId":"3148dfab-79e4-480a-e740-bdd81ad53380","execution":{"iopub.status.busy":"2022-08-18T11:27:16.315150Z","iopub.execute_input":"2022-08-18T11:27:16.316112Z","iopub.status.idle":"2022-08-18T11:27:16.426448Z","shell.execute_reply.started":"2022-08-18T11:27:16.316073Z","shell.execute_reply":"2022-08-18T11:27:16.425333Z"},"trusted":true},"execution_count":50,"outputs":[]},{"cell_type":"markdown","source":"### Generate and show some images.\n","metadata":{"id":"-I8PDocbHQiN"}},{"cell_type":"code","source":"# Generate 1000 images and make a grid to save them.\nn_output = 1000\nz_sample = Variable(torch.randn(n_output, z_dim)).cuda()\nimgs_sample = (G(z_sample).data + 1) / 2.0\nlog_dir = os.path.join(workspace_dir, 'logs')\nfilename = os.path.join(log_dir, 'result.jpg')\ntorchvision.utils.save_image(imgs_sample, filename, nrow=10)\n\n# Show 32 of the images.\ngrid_img = torchvision.utils.make_grid(imgs_sample[:32].cpu(), nrow=10)\nplt.figure(figsize=(10,10))\nplt.imshow(grid_img.permute(1, 2, 0))\nplt.show()","metadata":{"id":"x-SYKrRea_-Q","outputId":"aba269bb-3ea7-4df9-a30a-569107f522c1"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Compress the generated images using **tar**.\n","metadata":{"id":"6B04ATOTHc4F"}},{"cell_type":"code","source":"# Save the generated images.\nos.makedirs('output', exist_ok=True)\nfor i in range(1000):\n    torchvision.utils.save_image(imgs_sample[i], f'output/{i+1}.jpg')\n  \n# Compress the images.\n%cd output\n!tar -zcf ../images.tgz *.jpg\n%cd ..","metadata":{"id":"mbcmoTQpz_yf","outputId":"c2ef944b-b701-47b2-94b8-80a7aab36163"},"execution_count":null,"outputs":[]}]}