{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## AG_News 新闻主题分类\n\n**所有样本均使用逗号作为分隔符，一共 3 列，分别对应 `类标（1 到 4）`、`标题` 和 `新闻描述`。**\n\n**Transformer** 有两层，一层是 **Encoder** 层，一层是 **Decoder** 层，文本分类任务中并没有解码这一过程，不需要 **Decoder** 层，所以在案例中只对 **Encoder** 建立了编码层。","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-08-12T02:08:59.468987Z","iopub.execute_input":"2022-08-12T02:08:59.469353Z","iopub.status.idle":"2022-08-12T02:09:01.548747Z","shell.execute_reply.started":"2022-08-12T02:08:59.469269Z","shell.execute_reply":"2022-08-12T02:09:01.548008Z"}}},{"cell_type":"code","source":"from collections import Counter\nfrom torchtext.vocab import vocab\nfrom torch.utils.data import DataLoader\nfrom torchtext.data.utils import get_tokenizer\nimport torch\nimport torch.nn as nn\n\n## MyTransformer\nfrom torch.nn.init import xavier_uniform_\nimport torch.nn.functional as F\nfrom torch.nn import Parameter\nimport torch.nn as nn\nimport copy\n\nimport re\nfrom tqdm import tqdm\nimport math\nimport os\nimport time\n\nimport pandas as pd\nimport numpy as np","metadata":{"execution":{"iopub.status.busy":"2022-08-12T08:52:08.771952Z","iopub.execute_input":"2022-08-12T08:52:08.772459Z","iopub.status.idle":"2022-08-12T08:52:10.882058Z","shell.execute_reply.started":"2022-08-12T08:52:08.772347Z","shell.execute_reply":"2022-08-12T08:52:10.880892Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data = pd.read_csv('../input/ag-news/ag_news_csv/train.csv',header=None)\ndata.head()","metadata":{"execution":{"iopub.status.busy":"2022-08-12T08:52:10.892266Z","iopub.execute_input":"2022-08-12T08:52:10.893293Z","iopub.status.idle":"2022-08-12T08:52:11.600711Z","shell.execute_reply.started":"2022-08-12T08:52:10.893253Z","shell.execute_reply":"2022-08-12T08:52:11.599748Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"data.shape","metadata":{"execution":{"iopub.status.busy":"2022-08-12T08:52:11.602470Z","iopub.execute_input":"2022-08-12T08:52:11.603189Z","iopub.status.idle":"2022-08-12T08:52:11.609869Z","shell.execute_reply.started":"2022-08-12T08:52:11.603136Z","shell.execute_reply":"2022-08-12T08:52:11.608951Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"### 一、数据集构建","metadata":{}},{"cell_type":"markdown","source":"**定义 tokenize：**对于英文语料的处理","metadata":{}},{"cell_type":"code","source":"def my_tokenizer(s):\n    ## 指定分词方法\n    tokenizer = get_tokenizer('basic_english')\n    return tokenizer(s)","metadata":{"execution":{"iopub.status.busy":"2022-08-12T08:52:11.612533Z","iopub.execute_input":"2022-08-12T08:52:11.613286Z","iopub.status.idle":"2022-08-12T08:52:11.620311Z","shell.execute_reply.started":"2022-08-12T08:52:11.613252Z","shell.execute_reply":"2022-08-12T08:52:11.619334Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"**定义字符串清理：** 原始语料中有很多奇奇怪怪的字符，因此还需要对其稍微做一点处理。例如①只保留字母、数字、以及常用标点；②全部转换为小写字等","metadata":{}},{"cell_type":"code","source":"def clean_str(string):\n    string = re.sub(\"[^A-Za-z0-9\\-\\?\\!\\.\\,]\", \" \", string).lower()\n    return string","metadata":{"execution":{"iopub.status.busy":"2022-08-12T08:52:11.623262Z","iopub.execute_input":"2022-08-12T08:52:11.623569Z","iopub.status.idle":"2022-08-12T08:52:11.632156Z","shell.execute_reply.started":"2022-08-12T08:52:11.623544Z","shell.execute_reply":"2022-08-12T08:52:11.630924Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"**建立词表：**通过 `torchtext.vocab` 中的 `Vocab` 方法来构建词典","metadata":{"execution":{"iopub.status.busy":"2022-08-12T02:14:19.752798Z","iopub.execute_input":"2022-08-12T02:14:19.754259Z","iopub.status.idle":"2022-08-12T02:14:19.759983Z","shell.execute_reply.started":"2022-08-12T02:14:19.754220Z","shell.execute_reply":"2022-08-12T02:14:19.759004Z"}}},{"cell_type":"code","source":"def build_vocab(tokenizer, filepath, min_freq, specials=None):\n    \"\"\"\n    根据给定的tokenizer和对应参数返回一个Vocab类\n    Args:\n        tokenizer:  分词器\n        filepath:  文本的路径\n        min_freq: 最小词频，去掉小于min_freq的词\n        specials: 特殊的字符，如<pad>，<unk>等\n    Returns:\n    \"\"\"\n    if specials is None:\n        specials = ['<unk>', '<pad>']\n    counter = Counter()\n    with open(filepath, encoding='utf8') as f:\n        for string_ in tqdm(f):\n            string_ = string_.strip().split('\",\"')[-1][:-1]      ## 新闻描述\n            counter.update(tokenizer(clean_str(string_)))        ## 清洗sentence并进行分词和词计数\n    return vocab(counter,min_freq=min_freq,specials=specials)    ## 返回词汇表    ","metadata":{"execution":{"iopub.status.busy":"2022-08-12T08:52:11.633540Z","iopub.execute_input":"2022-08-12T08:52:11.634104Z","iopub.status.idle":"2022-08-12T08:52:11.643713Z","shell.execute_reply.started":"2022-08-12T08:52:11.634069Z","shell.execute_reply":"2022-08-12T08:52:11.642834Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"定义一个类，并在类的初始化过程中根据训练语料完成字典的构建","metadata":{}},{"cell_type":"code","source":"class LoadSentenceClassificationDataset():\n    def __init__(self, train_file_path=None,  # 训练集路径\n                 tokenizer=None,\n                 batch_size=20,\n                 min_freq=1,  # 最小词频，去掉小于min_freq的词\n                 max_sen_len='same'):  # 最大句子长度，默认设置其长度为整个数据集中最长样本的长度\n        max_sen_len = None  # 时，表示按每个batch中最长的样本长度进行padding\n        # 根据训练预料建立字典\n        self.tokenizer = tokenizer\n        self.min_freq = min_freq\n        self.specials = ['<unk>', '<pad>']\n        self.vocab = build_vocab(self.tokenizer,\n                                 filepath=train_file_path,\n                                 min_freq=self.min_freq,\n                                 specials=self.specials)\n        self.PAD_IDX = self.vocab['<pad>']\n        self.UNK_IDX = self.vocab['<unk>']\n        self.batch_size = batch_size\n        self.max_sen_len = max_sen_len\n    \n    ## 转换为 Token 序列,构建的字典后，便可以通过如下函数来将训练集和测试集转换成 Token 序列\n    def data_process(self, filepath):\n        \"\"\"\n        将每一句话中的每一个词根据字典转换成索引的形式，同时返回所有样本中最长样本的长度\n        :param filepath: 数据集路径\n        :return:\n        \"\"\"\n        ## \n        str_list = self.vocab.get_itos()\n        \n        raw_iter = open(filepath,encoding='utf8').readlines()\n        data = []\n        max_len = 0\n        for raw in tqdm(raw_iter, ncols=80):\n            line = raw.rstrip(\"\\n\").split('\",\"')\n            s, l = line[-1][:-1], line[0][1:]\n            s = clean_str(s)\n            \n            ## str2index\n            try:\n                tensor_ = torch.tensor([self.vocab[token] for token in self.tokenizer(s)], dtype=torch.long) ## 如果在词汇表中找不到词会报错\n#                 print('当前s中存在不在词汇表中的词汇：{}'.format(s))\n            except:\n                ## 如果数据不存在之前建立的词汇表中，就使用 <unk> 来代替，<unk> 的 index 为 0\n                tensor_ = torch.tensor([self.vocab[token] if token in str_list else 0 for token in self.tokenizer(s) ] , dtype=torch.long)     \n    \n            l = torch.tensor(int(l) - 1, dtype=torch.long)\n            max_len = max(max_len, tensor_.size(0))\n            data.append((tensor_, l))\n        return data, max_len\n    \n    ## 构造一个 DataLoader 迭代器\n    def load_train_val_test_data(self, train_file_paths, test_file_paths):\n        train_data, max_sen_len = self.data_process(train_file_paths)  # 得到处理好的所有样本\n        if self.max_sen_len == 'same':\n            self.max_sen_len = max_sen_len\n        test_data, _ = self.data_process(test_file_paths)\n        train_iter = DataLoader(train_data, batch_size=self.batch_size,  # 构造DataLoader,并指定填充\n                                shuffle=True, collate_fn=self.generate_batch)\n        test_iter = DataLoader(test_data, batch_size=self.batch_size,\n                               shuffle=True, collate_fn=self.generate_batch)\n        return train_iter, test_iter\n    \n    ## 对每个 batch 中的数据集进行 padding 处理：\n    def generate_batch(self, data_batch):\n        batch_sentence, batch_label = [], []\n        for (sen, label) in data_batch:  # 开始对一个batch中的每一个样本进行处理。\n            batch_sentence.append(sen)\n            batch_label.append(label)\n        batch_sentence = pad_sequence(batch_sentence,  # [batch_size,max_len]\n                                      padding_value=self.PAD_IDX,\n                                      batch_first=False,\n                                      max_len=self.max_sen_len)\n        batch_label = torch.tensor(batch_label, dtype=torch.long)\n        return batch_sentence, batch_label","metadata":{"execution":{"iopub.status.busy":"2022-08-12T08:52:11.647038Z","iopub.execute_input":"2022-08-12T08:52:11.647345Z","iopub.status.idle":"2022-08-12T08:52:11.663442Z","shell.execute_reply.started":"2022-08-12T08:52:11.647321Z","shell.execute_reply":"2022-08-12T08:52:11.662577Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"由于对于不同的样本来说其对应的序列长度通常来说都是不同的，但是在将数据输入到相应模型时却需要保持同样的长度。因此在这里我们就需要对 `Token` 序列化后的样本进行 `padding` 处理","metadata":{}},{"cell_type":"code","source":"def pad_sequence(sequences, batch_first=False, max_len=None, padding_value=0):\n    \"\"\"\n    对一个List中的元素进行padding\n        sequences:\n        batch_first: 是否把batch_size放到第一个维度\n        padding_value:\n        max_len : 最大句子长度，默认为None，即在每个batch中以最长样本的长度对其它样本进行padding；\n        当指定max_len的长度小于一个batch中某个样本的长度，那么在这个batch中还是会以最长样本的长度对其它样本进行padding\n        建议指定max_len的值为整个数据集中最长样本的长度\n    Returns:\n    \"\"\"\n    max_size = sequences[0].size()\n    trailing_dims = max_size[1:]\n    length = max_len\n    max_len = max([s.size(0) for s in sequences])\n    if length is not None:\n        max_len = max(length, max_len)\n    if batch_first:\n        out_dims = (len(sequences), max_len) + trailing_dims\n    else:\n        out_dims = (max_len, len(sequences)) + trailing_dims\n    out_tensor = sequences[0].data.new(*out_dims).fill_(padding_value)\n    for i, tensor in enumerate(sequences):\n        length = tensor.size(0)\n        # use index notation to prevent duplicate references to the tensor\n        if batch_first:\n            out_tensor[i, :length, ...] = tensor\n        else:\n            out_tensor[:length, i, ...] = tensor\n    return out_tensor","metadata":{"execution":{"iopub.status.busy":"2022-08-12T08:52:11.913125Z","iopub.execute_input":"2022-08-12T08:52:11.913940Z","iopub.status.idle":"2022-08-12T08:52:11.923145Z","shell.execute_reply.started":"2022-08-12T08:52:11.913871Z","shell.execute_reply":"2022-08-12T08:52:11.922146Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"##### **开始数据的处理（演示）**","metadata":{}},{"cell_type":"code","source":"# path = \"../input/ag-news/ag_news_csv/train.csv\"\n# data_loader = LoadSentenceClassificationDataset(train_file_path=path,tokenizer=my_tokenizer,max_sen_len=None)\n\n# ## data_loader.vocab.get_stoi() 可以对 data_loader 中的 vocab 进行相对应的词汇表操作 https://pytorch.org/text/stable/vocab.html\n# ## data_loader.vocab['hello']   ## 对某个字进行编码 str2index\n\n# data, max_len = data_loader.data_process(path)\n# train_iter, test_iter = data_loader.load_train_val_test_data(path, path)\n# for sample, label in train_iter:\n#     print(sample.shape)  # [seq_len,batch_size]","metadata":{"execution":{"iopub.status.busy":"2022-08-12T08:52:12.222869Z","iopub.execute_input":"2022-08-12T08:52:12.223432Z","iopub.status.idle":"2022-08-12T08:52:12.227949Z","shell.execute_reply.started":"2022-08-12T08:52:12.223400Z","shell.execute_reply":"2022-08-12T08:52:12.226997Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"# a = 'ap - southern california s smog-fighting agency went after emissions of the bovine variety friday, adopting the nation s first rules to reduce air pollution from dairy cow manure.'","metadata":{"execution":{"iopub.status.busy":"2022-08-12T08:52:12.352247Z","iopub.execute_input":"2022-08-12T08:52:12.352850Z","iopub.status.idle":"2022-08-12T08:52:12.358561Z","shell.execute_reply.started":"2022-08-12T08:52:12.352820Z","shell.execute_reply":"2022-08-12T08:52:12.357439Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"# b = data_loader.vocab.get_itos()","metadata":{"execution":{"iopub.status.busy":"2022-08-12T08:52:12.443922Z","iopub.execute_input":"2022-08-12T08:52:12.444199Z","iopub.status.idle":"2022-08-12T08:52:12.448609Z","shell.execute_reply.started":"2022-08-12T08:52:12.444175Z","shell.execute_reply":"2022-08-12T08:52:12.447347Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"# 'aspo' in b","metadata":{"execution":{"iopub.status.busy":"2022-08-12T08:52:12.521852Z","iopub.execute_input":"2022-08-12T08:52:12.522436Z","iopub.status.idle":"2022-08-12T08:52:12.526363Z","shell.execute_reply.started":"2022-08-12T08:52:12.522409Z","shell.execute_reply":"2022-08-12T08:52:12.525196Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"# tensor_ = torch.tensor([data_loader.vocab[token] if token in b else 0 for token in my_tokenizer(a) ]  , dtype=torch.long)","metadata":{"execution":{"iopub.status.busy":"2022-08-12T08:52:12.603958Z","iopub.execute_input":"2022-08-12T08:52:12.604652Z","iopub.status.idle":"2022-08-12T08:52:12.609043Z","shell.execute_reply.started":"2022-08-12T08:52:12.604615Z","shell.execute_reply":"2022-08-12T08:52:12.608028Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"# raw_iter = open(config.test_corpus_file_paths,encoding='utf8').readlines()\n# data = []\n# max_len = 0\n# i = 0\n# for raw in tqdm(raw_iter, ncols=80):\n#     line = raw.rstrip(\"\\n\").split('\",\"')\n#     s, l = line[-1][:-1], line[0][1:]\n#     s = clean_str(s)\n#     print(s)\n#     i = i+1\n#     if i >5:\n#         break\n#     tensor_ = torch.tensor([data_loader.vocab[token] for token in my_tokenizer(s)], dtype=torch.long)","metadata":{"execution":{"iopub.status.busy":"2022-08-12T08:52:12.718621Z","iopub.execute_input":"2022-08-12T08:52:12.719271Z","iopub.status.idle":"2022-08-12T08:52:12.723797Z","shell.execute_reply.started":"2022-08-12T08:52:12.719235Z","shell.execute_reply":"2022-08-12T08:52:12.722851Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"# train_iter, test_iter = data_loader.load_train_val_test_data(config.train_corpus_file_paths, config.test_corpus_file_paths)","metadata":{"execution":{"iopub.status.busy":"2022-08-12T08:52:12.863909Z","iopub.execute_input":"2022-08-12T08:52:12.864762Z","iopub.status.idle":"2022-08-12T08:52:12.868990Z","shell.execute_reply.started":"2022-08-12T08:52:12.864728Z","shell.execute_reply":"2022-08-12T08:52:12.867835Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"## build_vocab 中做的事：\n# counter = Counter()\n# with open(path, encoding='utf8') as f:\n#     for string_ in tqdm(f):\n#         print(string_)\n#         string_ = string_.strip().split('\",\"')[-1][:-1]  # 新闻描述\n#         print(string_)\n#         print(clean_str(string_))\n#         print(my_tokenizer(clean_str(string_)))\n#         counter.update(my_tokenizer(clean_str(string_)))        \n#         print(counter.most_common)\n#         break","metadata":{"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2022-08-12T08:52:13.192398Z","iopub.execute_input":"2022-08-12T08:52:13.193326Z","iopub.status.idle":"2022-08-12T08:52:13.198821Z","shell.execute_reply.started":"2022-08-12T08:52:13.193280Z","shell.execute_reply":"2022-08-12T08:52:13.197494Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":"### 二、定义模型的网络结构","metadata":{}},{"cell_type":"markdown","source":"**Embedding：**包含`token enbedding` 和 `PositionalEncoding` 两种","metadata":{}},{"cell_type":"code","source":"class PositionalEncoding(nn.Module):\n    r\"\"\"Inject some information about the relative or absolute position of the tokens\n        in the sequence. The positional encodings have the same dimension as\n        the embeddings, so that the two can be summed. Here, we use sine and cosine\n        functions of different frequencies.\n    .. math::\n        \\text{PosEncoder}(pos, 2i) = sin(pos/10000^(2i/d_model))\n        \\text{PosEncoder}(pos, 2i+1) = cos(pos/10000^(2i/d_model))\n        \\text{where pos is the word position and i is the embed idx)\n    Args:\n        d_model: the embed dim (required).\n        dropout: the dropout value (default=0.1).\n        max_len: the max. length of the incoming sequence (default=5000).\n    Examples:\n        #>>> pos_encoder = PositionalEncoding(d_model)\n    \"\"\"\n\n    def __init__(self, d_model, dropout=0.1, max_len=5000):\n        super(PositionalEncoding, self).__init__()\n        self.dropout = nn.Dropout(p=dropout)\n        pe = torch.zeros(max_len, d_model)  # [max_len, d_model]\n        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)  # [max_len, 1]\n        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))  # [d_model/2]\n        pe[:, 0::2] = torch.sin(position * div_term)  # [max_len, d_model/2]\n        pe[:, 1::2] = torch.cos(position * div_term)\n        pe = pe.unsqueeze(0).transpose(0, 1)  # [max_len, 1, d_model]\n        self.register_buffer('pe', pe)\n\n    def forward(self, x):  # [x_len, batch_size, d_model]\n        \"\"\"\n        :param x: [x_len, batch_size, emb_size]\n        :return: [x_len, batch_size, emb_size]\n        \"\"\"\n        x = x + self.pe[:x.size(0), :]  # [batch_size, max_len, d_model]\n        return self.dropout(x)\n\n\nclass TokenEmbedding(nn.Module):\n    def __init__(self, vocab_size: int, emb_size):\n        super(TokenEmbedding, self).__init__()\n        self.embedding = nn.Embedding(vocab_size, emb_size)\n        self.emb_size = emb_size\n\n    \"\"\"\n        :param tokens: shape : [len, batch_size]\n        :return: shape: [len, batch_size, emb_size]\n        \"\"\"\n\n    def forward(self, tokens):\n        return self.embedding(tokens.long()) * math.sqrt(self.emb_size)","metadata":{"execution":{"iopub.status.busy":"2022-08-12T08:52:13.552928Z","iopub.execute_input":"2022-08-12T08:52:13.553700Z","iopub.status.idle":"2022-08-12T08:52:13.567163Z","shell.execute_reply.started":"2022-08-12T08:52:13.553665Z","shell.execute_reply":"2022-08-12T08:52:13.566154Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"markdown","source":"**MyTransformer**","metadata":{}},{"cell_type":"code","source":"is_print_shape = False\n\nclass MyTransformer(nn.Module):\n    def __init__(self, d_model=512, nhead=8, num_encoder_layers=6,\n                 num_decoder_layers=6, dim_feedforward=2048, dropout=0.1,\n                 ):\n        super(MyTransformer, self).__init__()\n\n        \"\"\"\n        :param d_model:  d_k = d_v = d_model/nhead = 64, 模型中向量的维度，论文默认值为 512\n        :param nhead:               多头注意力机制中多头的数量，论文默认为值 8\n        :param num_encoder_layers:  encoder堆叠的数量，也就是论文中的N，论文默认值为6\n        :param num_decoder_layers:  decoder堆叠的数量，也就是论文中的N，论文默认值为6\n        :param dim_feedforward:     全连接中向量的维度，论文默认值为 2048\n        :param dropout:             丢弃率，论文中的默认值为 0.1\n        \"\"\"\n\n        #  ================ 编码部分 =====================\n        encoder_layer = MyTransformerEncoderLayer(d_model, nhead, dim_feedforward, dropout)\n        encoder_norm = nn.LayerNorm(d_model)\n        self.encoder = MyTransformerEncoder(encoder_layer, num_encoder_layers, encoder_norm)\n\n        # ================ 解码部分 =====================\n        decoder_layer = MyTransformerDecoderLayer(d_model, nhead, dim_feedforward, dropout)\n        decoder_norm = nn.LayerNorm(d_model)\n        self.decoder = MyTransformerDecoder(decoder_layer, num_decoder_layers, decoder_norm)\n\n        self._reset_parameters()\n\n        self.d_model = d_model\n        self.nhead = nhead\n\n    def _reset_parameters(self):\n        r\"\"\"Initiate parameters in the transformer model.\"\"\"\n        \"\"\"\n        初始化\n        \"\"\"\n        for p in self.parameters():\n            if p.dim() > 1:\n                xavier_uniform_(p)\n\n    def forward(self, src, tgt, src_mask=None, tgt_mask=None,\n                memory_mask=None, src_key_padding_mask=None,\n                tgt_key_padding_mask=None, memory_key_padding_mask=None):\n        \"\"\"\n        :param src:   [src_len,batch_size,embed_dim]\n        :param tgt:  [tgt_len, batch_size, embed_dim]\n        :param src_mask:  None\n        :param tgt_mask:  [tgt_len, tgt_len]\n        :param memory_mask: None\n        :param src_key_padding_mask: [batch_size, src_len]\n        :param tgt_key_padding_mask: [batch_size, tgt_len]\n        :param memory_key_padding_mask:  [batch_size, src_len]\n        :return: [tgt_len, batch_size, num_heads * kdim] <==> [tgt_len,batch_size,embed_dim]\n        \"\"\"\n        memory = self.encoder(src, mask=src_mask, src_key_padding_mask=src_key_padding_mask)\n        # [src_len, batch_size, num_heads * kdim] <==> [src_len,batch_size,embed_dim]\n        output = self.decoder(tgt=tgt, memory=memory, tgt_mask=tgt_mask, memory_mask=memory_mask,\n                              tgt_key_padding_mask=tgt_key_padding_mask,\n                              memory_key_padding_mask=memory_key_padding_mask)\n        return output  # [tgt_len, batch_size, num_heads * kdim] <==> [tgt_len,batch_size,embed_dim]\n\n    def generate_square_subsequent_mask(self, sz):\n        r\"\"\"Generate a square mask for the sequence. The masked positions are filled with float('-inf').\n            Unmasked positions are filled with float(0.0).\n        \"\"\"\n        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n        return mask  # [sz,sz]\n\n\nclass MyTransformerEncoderLayer(nn.Module):\n    def __init__(self, d_model, nhead, dim_feedforward=2048, dropout=0.1):\n        super(MyTransformerEncoderLayer, self).__init__()\n        \"\"\"\n        :param d_model:         d_k = d_v = d_model/nhead = 64, 模型中向量的维度，论文默认值为 512\n        :param nhead:           多头注意力机制中多头的数量，论文默认为值 8\n        :param dim_feedforward: 全连接中向量的维度，论文默认值为 2048\n        :param dropout:         丢弃率，论文中的默认值为 0.1    \n        \"\"\"\n        self.self_attn = MyMultiheadAttention(d_model, nhead, dropout=dropout)\n\n        # Implementation of Feedforward model\n        self.dropout1 = nn.Dropout(dropout)\n        self.norm1 = nn.LayerNorm(d_model)\n\n        self.linear1 = nn.Linear(d_model, dim_feedforward)\n        self.dropout = nn.Dropout(dropout)\n        self.linear2 = nn.Linear(dim_feedforward, d_model)\n        self.activation = F.relu\n\n        self.dropout2 = nn.Dropout(dropout)\n        self.norm2 = nn.LayerNorm(d_model)\n\n    def forward(self, src, src_mask=None, src_key_padding_mask=None):\n        \"\"\"\n        :param src: 编码部分的输入，形状为 [src_len,batch_size, embed_dim]\n        :param src_mask:  编码部分输入的padding情况，形状为 [batch_size, src_len]\n        :return:\n        \"\"\"\n        src2 = self.self_attn(src, src, src, attn_mask=src_mask,\n                              key_padding_mask=src_key_padding_mask, )[0]  # 计算多头注意力\n        # src2: [src_len,batch_size,num_heads*kdim] num_heads*kdim = embed_dim\n        src = src + self.dropout1(src2)  # 残差连接\n        src = self.norm1(src)  # [src_len,batch_size,num_heads*kdim]\n\n        src2 = self.activation(self.linear1(src))  # [src_len,batch_size,dim_feedforward]\n        src2 = self.linear2(self.dropout(src2))  # [src_len,batch_size,num_heads*kdim]\n        src = src + self.dropout2(src2)\n        src = self.norm2(src)\n        return src  # [src_len, batch_size, num_heads * kdim] <==> [src_len,batch_size,embed_dim]\n\n\nclass MyTransformerEncoder(nn.Module):\n    def __init__(self, encoder_layer, num_layers, norm=None):\n        super(MyTransformerEncoder, self).__init__()\n        \"\"\"\n        encoder_layer: 就是包含有多头注意力机制的一个编码层\n        num_layers: 克隆得到多个encoder layers 论文中默认为6\n        norm: 归一化层\n        \"\"\"\n        self.layers = _get_clones(encoder_layer, num_layers)  # 克隆得到多个encoder layers 论文中默认为6\n        self.num_layers = num_layers\n        self.norm = norm\n\n    def forward(self, src, mask=None, src_key_padding_mask=None):\n        \"\"\"\n        :param src: 编码部分的输入，形状为 [src_len,batch_size, embed_dim]\n        :param mask:  编码部分输入的padding情况，形状为 [batch_size, src_len]\n        :return:# [src_len, batch_size, num_heads * kdim] <==> [src_len,batch_size,embed_dim]\n        \"\"\"\n        output = src\n        for mod in self.layers:\n            output = mod(output, src_mask=mask,\n                         src_key_padding_mask=src_key_padding_mask)  # 多个encoder layers层堆叠后的前向传播过程\n        if self.norm is not None:\n            output = self.norm(output)\n        return output  # [src_len, batch_size, num_heads * kdim] <==> [src_len,batch_size,embed_dim]\n\n\ndef _get_clones(module, N):\n    return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])\n\n\nclass MyTransformerDecoderLayer(nn.Module):\n    def __init__(self, d_model, nhead, dim_feedforward=2048, dropout=0.1):\n        super(MyTransformerDecoderLayer, self).__init__()\n        \"\"\"\n        :param d_model:         d_k = d_v = d_model/nhead = 64, 模型中向量的维度，论文默认值为 512\n        :param nhead:           多头注意力机制中多头的数量，论文默认为值 8\n        :param dim_feedforward: 全连接中向量的维度，论文默认值为 2048\n        :param dropout:         丢弃率，论文中的默认值为 0.1    \n        \"\"\"\n        self.self_attn = MyMultiheadAttention(embed_dim=d_model, num_heads=nhead, dropout=dropout)\n        # 解码部分输入序列之间的多头注意力（也就是论文结构图中的Masked Multi-head attention)\n        self.multihead_attn = MyMultiheadAttention(embed_dim=d_model, num_heads=nhead, dropout=dropout)\n        # 编码部分输出（memory）和解码部分之间的多头注意力机制。\n        # Implementation of Feedforward model\n\n        self.linear1 = nn.Linear(d_model, dim_feedforward)\n        self.dropout = nn.Dropout(dropout)\n        self.linear2 = nn.Linear(dim_feedforward, d_model)\n\n        self.norm1 = nn.LayerNorm(d_model)\n        self.norm2 = nn.LayerNorm(d_model)\n        self.norm3 = nn.LayerNorm(d_model)\n        self.dropout1 = nn.Dropout(dropout)\n        self.dropout2 = nn.Dropout(dropout)\n        self.dropout3 = nn.Dropout(dropout)\n\n        self.activation = F.relu\n\n    def forward(self, tgt, memory, tgt_mask=None, memory_mask=None, tgt_key_padding_mask=None,\n                memory_key_padding_mask=None):\n        \"\"\"\n        :param tgt:  解码部分的输入，形状为 [tgt_len,batch_size, embed_dim]\n        :param memory: 编码部分的输出（memory）, [src_len,batch_size,embed_dim]\n        :param tgt_mask: 注意力Mask输入，用于掩盖当前position之后的信息, [tgt_len, tgt_len]\n        :param memory_mask: 编码器-解码器交互时的注意力掩码，一般为None\n        :param tgt_key_padding_mask: 解码部分输入的padding情况，形状为 [batch_size, tgt_len]\n        :param memory_key_padding_mask: 编码部分输入的padding情况，形状为 [batch_size, src_len]\n        :return:\n        \"\"\"\n        tgt2 = self.self_attn(tgt, tgt, tgt,  # [tgt_len,batch_size, embed_dim]\n                              attn_mask=tgt_mask,\n                              key_padding_mask=tgt_key_padding_mask)[0]\n        # 解码部分输入序列之间'的多头注意力（也就是论文结构图中的Masked Multi-head attention)\n\n        tgt = tgt + self.dropout1(tgt2)  # 接着是残差连接\n        tgt = self.norm1(tgt)  # [tgt_len,batch_size, embed_dim]\n\n        tgt2 = self.multihead_attn(tgt, memory, memory,  # [tgt_len, batch_size, embed_dim]\n                                   attn_mask=memory_mask,\n                                   key_padding_mask=memory_key_padding_mask)[0]\n\n        # 解码部分的输入经过多头注意力后同编码部分的输出（memory）通过多头注意力机制进行交互\n        tgt = tgt + self.dropout2(tgt2)  # 残差连接\n        tgt = self.norm2(tgt)  # [tgt_len, batch_size, embed_dim]\n\n        tgt2 = self.activation(self.linear1(tgt))  # [tgt_len, batch_size, dim_feedforward]\n        tgt2 = self.linear2(self.dropout(tgt2))  # [tgt_len, batch_size, embed_dim]\n        # 最后的两层全连接\n        tgt = tgt + self.dropout3(tgt2)\n        tgt = self.norm3(tgt)\n        return tgt  # [tgt_len, batch_size, num_heads * kdim] <==> [tgt_len,batch_size,embed_dim]\n\n\nclass MyTransformerDecoder(nn.Module):\n    def __init__(self, decoder_layer, num_layers, norm=None):\n        super(MyTransformerDecoder, self).__init__()\n        self.layers = _get_clones(decoder_layer, num_layers)\n        self.num_layers = num_layers\n        self.norm = norm\n\n    def forward(self, tgt, memory, tgt_mask=None, memory_mask=None, tgt_key_padding_mask=None,\n                memory_key_padding_mask=None):\n        \"\"\"\n        :param tgt: 解码部分的输入，形状为 [tgt_len,batch_size, embed_dim]\n        :param memory: 编码部分最后一层的输出 [src_len,batch_size, embed_dim]\n        :param tgt_mask: 注意力Mask输入，用于掩盖当前position之后的信息, [tgt_len, tgt_len]\n        :param memory_mask: 编码器-解码器交互时的注意力掩码，一般为None\n        :param tgt_key_padding_mask: 解码部分输入的padding情况，形状为 [batch_size, tgt_len]\n        :param memory_key_padding_mask: 编码部分输入的padding情况，形状为 [batch_size, src_len]\n        :return:\n        \"\"\"\n        output = tgt  # [tgt_len,batch_size, embed_dim]\n\n        for mod in self.layers:  # 这里的layers就是N层解码层堆叠起来的\n            output = mod(output, memory,\n                         tgt_mask=tgt_mask,\n                         memory_mask=memory_mask,\n                         tgt_key_padding_mask=tgt_key_padding_mask,\n                         memory_key_padding_mask=memory_key_padding_mask)\n        if self.norm is not None:\n            output = self.norm(output)\n\n        return output  # [tgt_len, batch_size, num_heads * kdim] <==> [tgt_len,batch_size,embed_dim]\n\n\nclass MyMultiheadAttention(nn.Module):\n    \"\"\"\n    多头注意力机制的计算公式为（就是论文第5页的公式）：\n    .. math::\n        \\text{MultiHead}(Q, K, V) = \\text{Concat}(head_1,\\dots,head_h)W^O\n        \\text{where} head_i = \\text{Attention}(QW_i^Q, KW_i^K, VW_i^V)\n    \"\"\"\n\n    def __init__(self, embed_dim, num_heads, dropout=0., bias=True):\n        super(MyMultiheadAttention, self).__init__()\n        \"\"\"\n        :param embed_dim:   词嵌入的维度，也就是前面的d_model参数，论文中的默认值为512\n        :param num_heads:   多头注意力机制中多头的数量，也就是前面的nhead参数， 论文默认值为 8\n        :param dropout:     \n        :param bias:        最后对多头的注意力（组合）输出进行线性变换时，是否使用偏置\n        \"\"\"\n        self.embed_dim = embed_dim  # 前面的d_model参数\n        self.head_dim = embed_dim // num_heads  # head_dim 指的就是d_k,d_v\n        self.kdim = self.head_dim\n        self.vdim = self.head_dim\n\n        self.num_heads = num_heads  # 多头个数\n        self.dropout = dropout\n\n        assert self.head_dim * num_heads == self.embed_dim, \"embed_dim 除以 num_heads必须为整数\"\n        # 上面的限制条件就是论文中的  d_k = d_v = d_model/n_head 条件\n\n        self.q_proj = nn.Linear(embed_dim, embed_dim, bias=bias)  # embed_dim = kdim * num_heads\n        # 这里第二个维度之所以是embed_dim，实际上这里是同时初始化了num_heads个W_q堆叠起来的, 也就是num_heads个头\n        self.k_proj = nn.Linear(embed_dim, embed_dim, bias=bias)  # W_k,  embed_dim = kdim * num_heads\n        self.v_proj = nn.Linear(embed_dim, embed_dim, bias=bias)  # W_v,  embed_dim = vdim * num_heads\n        self.out_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n        # 最后将所有的Z组合起来的时候，也是一次性完成， embed_dim = vdim * num_heads\n        self._reset_parameters()\n\n    def _reset_parameters(self):\n        \"\"\"\n        以特定方式来初始化参数\n        :return:\n        \"\"\"\n        for p in self.parameters():\n            if p.dim() > 1:\n                xavier_uniform_(p)\n\n    def forward(self, query, key, value, attn_mask=None, key_padding_mask=None):\n        \"\"\"\n        在论文中，编码时query, key, value 都是同一个输入， 解码时 输入的部分也都是同一个输入，\n        解码和编码交互时 key,value指的是 memory, query指的是tgt\n        :param query: # [tgt_len, batch_size, embed_dim], tgt_len 表示目标序列的长度\n        :param key:  #  [src_len, batch_size, embed_dim], src_len 表示源序列的长度\n        :param value: # [src_len, batch_size, embed_dim], src_len 表示源序列的长度\n        :param attn_mask: # [tgt_len,src_len] or [num_heads*batch_size,tgt_len, src_len]\n                一般只在解码时使用，为了并行一次喂入所有解码部分的输入，所以要用mask来进行掩盖当前时刻之后的位置信息\n        :param key_padding_mask: [batch_size, src_len], src_len 表示源序列的长度\n        :return:\n        attn_output: [tgt_len, batch_size, embed_dim]\n        attn_output_weights: # [batch_size, tgt_len, src_len]\n        \"\"\"\n        return multi_head_attention_forward(query, key, value, self.num_heads,\n                                            self.dropout,\n                                            out_proj=self.out_proj,\n                                            training=self.training,\n                                            key_padding_mask=key_padding_mask,\n                                            q_proj=self.q_proj,\n                                            k_proj=self.k_proj,\n                                            v_proj=self.v_proj,\n                                            attn_mask=attn_mask)\n\n\ndef multi_head_attention_forward(query,  # [tgt_len,batch_size, embed_dim]\n                                 key,  # [src_len, batch_size, embed_dim]\n                                 value,  # [src_len, batch_size, embed_dim]\n                                 num_heads,\n                                 dropout_p,\n                                 out_proj,  # [embed_dim = vdim * num_heads, embed_dim = vdim * num_heads]\n                                 training=True,\n                                 key_padding_mask=None,  # [batch_size,src_len/tgt_len]\n                                 q_proj=None,  # [embed_dim,kdim * num_heads]\n                                 k_proj=None,  # [embed_dim, kdim * num_heads]\n                                 v_proj=None,  # [embed_dim, vdim * num_heads]\n                                 attn_mask=None,  # [tgt_len,src_len] or [num_heads*batch_size,tgt_len, src_len]\n                                 ):\n    q = q_proj(query)\n    #  [tgt_len,batch_size, embed_dim] x [embed_dim,kdim * num_heads] = [tgt_len,batch_size,kdim * num_heads]\n\n    k = k_proj(key)\n    # [src_len, batch_size, embed_dim] x [embed_dim, kdim * num_heads] = [src_len, batch_size, kdim * num_heads]\n\n    v = v_proj(value)\n    # [src_len, batch_size, embed_dim] x [embed_dim, vdim * num_heads] = [src_len, batch_size, vdim * num_heads]\n    if is_print_shape:\n        print(\"\" + \"=\" * 80)\n        print(\"进入多头注意力计算:\")\n        print(\n            f\"\\t 多头num_heads = {num_heads}, d_model={query.size(-1)}, d_k = d_v = d_model/num_heads={query.size(-1) // num_heads}\")\n        print(f\"\\t query的shape([tgt_len, batch_size, embed_dim]):{query.shape}\")\n        print(f\"\\t  W_q 的shape([embed_dim,kdim * num_heads]):{q_proj.weight.shape}\")\n        print(f\"\\t   Q  的shape([tgt_len, batch_size,kdim * num_heads]):{q.shape}\")\n        print(\"\\t\" + \"-\" * 70)\n\n        print(f\"\\t  key 的shape([src_len,batch_size, embed_dim]):{key.shape}\")\n        print(f\"\\t  W_k 的shape([embed_dim,kdim * num_heads]):{k_proj.weight.shape}\")\n        print(f\"\\t   K  的shape([src_len,batch_size,kdim * num_heads]):{k.shape}\")\n        print(\"\\t\" + \"-\" * 70)\n\n        print(f\"\\t value的shape([src_len,batch_size, embed_dim]):{value.shape}\")\n        print(f\"\\t  W_v 的shape([embed_dim,vdim * num_heads]):{v_proj.weight.shape}\")\n        print(f\"\\t   V  的shape([src_len,batch_size,vdim * num_heads]):{v.shape}\")\n        print(\"\\t\" + \"-\" * 70)\n        print(\"\\t ***** 注意，这里的W_q, W_k, W_v是多个head同时进行计算的. 因此，Q,K,V分别也是包含了多个head的q,k,v堆叠起来的结果 *****\")\n\n    tgt_len, bsz, embed_dim = query.size()  # [tgt_len,batch_size, embed_dim]\n    src_len = key.size(0)\n    head_dim = embed_dim // num_heads  # num_heads * head_dim = embed_dim\n    scaling = float(head_dim) ** -0.5\n    q = q * scaling  # [query_len,batch_size,kdim * num_heads]\n\n    if attn_mask is not None:  # [tgt_len,src_len] or [num_heads*batch_size,tgt_len, src_len]\n        if attn_mask.dim() == 2:\n            attn_mask = attn_mask.unsqueeze(0)  # [1, tgt_len,src_len]\n            if list(attn_mask.size()) != [1, query.size(0), key.size(0)]:\n                raise RuntimeError('The size of the 2D attn_mask is not correct.')\n        elif attn_mask.dim() == 3:\n            if list(attn_mask.size()) != [bsz * num_heads, query.size(0), key.size(0)]:\n                raise RuntimeError('The size of the 3D attn_mask is not correct.')\n        # 现在 atten_mask 的维度就变成了3D\n\n    q = q.contiguous().view(tgt_len, bsz * num_heads, head_dim).transpose(0, 1)\n    # [batch_size * num_heads,tgt_len,kdim]\n    # 因为前面是num_heads个头一起参与的计算，所以这里要进行一下变形，以便于后面计算。 且同时交换了0，1两个维度\n    k = k.contiguous().view(-1, bsz * num_heads, head_dim).transpose(0, 1)  # [batch_size * num_heads,src_len,kdim]\n    v = v.contiguous().view(-1, bsz * num_heads, head_dim).transpose(0, 1)  # [batch_size * num_heads,src_len,vdim]\n    attn_output_weights = torch.bmm(q, k.transpose(1, 2))\n    # [batch_size * num_heads,tgt_len,kdim] x [batch_size * num_heads, kdim, src_len]\n    # =  [batch_size * num_heads, tgt_len, src_len]  这就num_heads个QK相乘后的注意力矩阵\n\n    if attn_mask is not None:\n        attn_output_weights += attn_mask  # [batch_size * num_heads, tgt_len, src_len]\n\n    if key_padding_mask is not None:\n        attn_output_weights = attn_output_weights.view(bsz, num_heads, tgt_len, src_len)\n        # 变成 [batch_size, num_heads, tgt_len, src_len]的形状\n        attn_output_weights = attn_output_weights.masked_fill(\n            key_padding_mask.unsqueeze(1).unsqueeze(2),  # 扩展维度，从[batch_size,src_len]变成[batch_size,1,1,src_len]\n            float('-inf'))  #\n        attn_output_weights = attn_output_weights.view(bsz * num_heads, tgt_len,\n                                                       src_len)  # [batch_size * num_heads, tgt_len, src_len]\n\n    attn_output_weights = F.softmax(attn_output_weights, dim=-1)  # [batch_size * num_heads, tgt_len, src_len]\n    attn_output_weights = F.dropout(attn_output_weights, p=dropout_p, training=training)\n    attn_output = torch.bmm(attn_output_weights, v)\n    # Z = [batch_size * num_heads, tgt_len, src_len]  x  [batch_size * num_heads,src_len,vdim]\n    # = # [batch_size * num_heads,tgt_len,vdim]\n    # 这就num_heads个Attention(Q,K,V)结果\n\n    attn_output = attn_output.transpose(0, 1).contiguous().view(tgt_len, bsz, embed_dim)\n    # 先transpose成 [tgt_len, batch_size* num_heads ,kdim]\n    # 再view成 [tgt_len,batch_size,num_heads*kdim]\n    attn_output_weights = attn_output_weights.view(bsz, num_heads, tgt_len, src_len)\n\n    Z = out_proj(attn_output)\n    # 这里就是多个z  线性组合成Z  [tgt_len,batch_size,embed_dim]\n    if is_print_shape:\n        print(f\"\\t 多头注意力中,多头计算结束后的形状（堆叠）为([tgt_len,batch_size,num_heads*kdim]){attn_output.shape}\")\n        print(f\"\\t 多头计算结束后，再进行线性变换时的权重W_o的形状为([num_heads*vdim, num_heads*vdim  ]){out_proj.weight.shape}\")\n        print(f\"\\t 多头线性变化后的形状为([tgt_len,batch_size,embed_dim]) {Z.shape}\")\n    return Z, attn_output_weights.sum(dim=1) / num_heads  # average attention weights over heads\n","metadata":{"execution":{"iopub.status.busy":"2022-08-12T08:52:13.921530Z","iopub.execute_input":"2022-08-12T08:52:13.921920Z","iopub.status.idle":"2022-08-12T08:52:13.976246Z","shell.execute_reply.started":"2022-08-12T08:52:13.921875Z","shell.execute_reply":"2022-08-12T08:52:13.975237Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"markdown","source":"**Config：**    基于 `Transformer` 架构的类 `Translation` 模型配置类","metadata":{}},{"cell_type":"code","source":"class Config():\n    \"\"\"\n    基于Transformer架构的类Translation模型配置类\n    \"\"\"\n\n    def __init__(self):\n        #   数据集设置相关配置\n#         self.project_dir = os.path.dirname(os.path.abspath(__file__))\n#         self.dataset_dir = os.path.join(self.project_dir, 'data')\n#         self.train_corpus_file_paths = os.path.join(self.dataset_dir, 'ag_news_csv', 'train.csv')\n#         self.test_corpus_file_paths = os.path.join(self.dataset_dir, 'ag_news_csv', 'test.csv')\n\n        self.train_corpus_file_paths = '../input/ag-news/ag_news_csv/train.csv'\n        self.test_corpus_file_paths = '../input/ag-news/ag_news_csv/test.csv'\n\n\n        self.min_freq = 1\n        self.max_sen_len = None\n\n        #  模型相关配置\n        self.batch_size = 128\n        self.d_model = 512\n        self.num_head = 8\n        self.num_encoder_layers = 6\n        self.num_decoder_layers = 6\n        self.dim_feedforward = 512\n        self.dim_classification = 256\n        self.num_class = 4\n        self.dropout = 0.1\n        self.concat_type = 'avg'\n        self.beta1 = 0.9\n        self.beta2 = 0.98\n        self.epsilon = 10e-9\n        self.device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n        self.epochs = 10\n#         self.model_save_dir = os.path.join(self.project_dir, 'cache')\n        self.model_save_dir = os.path.join('./', 'cache')\n\n        self.model_save_per_epoch = 2\n        if not os.path.exists(self.model_save_dir):\n            os.makedirs(self.model_save_dir)","metadata":{"execution":{"iopub.status.busy":"2022-08-12T08:52:13.980425Z","iopub.execute_input":"2022-08-12T08:52:13.980711Z","iopub.status.idle":"2022-08-12T08:52:13.994204Z","shell.execute_reply.started":"2022-08-12T08:52:13.980664Z","shell.execute_reply":"2022-08-12T08:52:13.993047Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"class ClassificationModel(nn.Module):\n    def __init__(self, vocab_size=None,\n                 d_model=512, nhead=8,\n                 num_encoder_layers=6,\n                 dim_feedforward=2048,\n                 dim_classification=64,\n                 num_classification=4,\n                 dropout=0.1):\n        super(ClassificationModel, self).__init__()\n        \n        ##  Transformer 中的 Embedding 操作\n        self.pos_embedding = PositionalEncoding(d_model=d_model, dropout=dropout)\n        self.src_token_embedding = TokenEmbedding(vocab_size, d_model)\n        \n        ## 定义了 Transformer 中的 EncoderLayer\n        encoder_layer = MyTransformerEncoderLayer(d_model, nhead,\n                                                  dim_feedforward,\n                                                  dropout)\n        encoder_norm = nn.LayerNorm(d_model)\n        \n        ## Transformer 中的 Encoder\n        self.encoder = MyTransformerEncoder(encoder_layer,\n                                            num_encoder_layers, encoder_norm)\n        \n        ## 定义分类器\n        self.classifier = nn.Sequential(nn.Linear(d_model, dim_classification),\n                                        nn.Dropout(dropout),\n                                        nn.Linear(dim_classification, num_classification))\n    ## 前向传播\n    def forward(self,\n                src,  # [src_len, batch_size]\n                src_mask=None,\n                src_key_padding_mask=None,  # [batsh_size, src_len]\n                concat_type='sum'  # 解码之后取所有位置相加，还是最后一个位置作为输出\n                ):\n        # 执行编码器的前向传播过程\n        src_embed = self.src_token_embedding(src)  # [src_len, batch_size, embed_dim]\n        src_embed = self.pos_embedding(src_embed)  # [src_len, batch_size, embed_dim]\n        memory = self.encoder(src=src_embed,\n                              mask=src_mask,\n                              src_key_padding_mask=src_key_padding_mask)\n        # [src_len,batch_size,embed_dim]\n        \n        ## 选择以何种方式来选择分类器的输入，sum 或者 avg\n        if concat_type == 'sum':\n            memory = torch.sum(memory, dim=0)\n        elif concat_type == 'avg':\n            memory = torch.sum(memory, dim=0) / memory.size(0)\n        else:\n            memory = memory[-1, ::]  # 取最后一个时刻\n        # [src_len, batch_size, num_heads * kdim] <==> [src_len,batch_size,embed_dim]\n        out = self.classifier(memory)  # 输出logits\n        return out  # [batch_size, num_class]","metadata":{"execution":{"iopub.status.busy":"2022-08-12T08:52:14.165320Z","iopub.execute_input":"2022-08-12T08:52:14.165598Z","iopub.status.idle":"2022-08-12T08:52:14.177797Z","shell.execute_reply.started":"2022-08-12T08:52:14.165573Z","shell.execute_reply":"2022-08-12T08:52:14.174942Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"markdown","source":"### **3、定义训练函数**","metadata":{}},{"cell_type":"code","source":"class CustomSchedule(nn.Module):\n    def __init__(self, d_model, warmup_steps=4000):\n        super(CustomSchedule, self).__init__()\n        self.d_model = torch.tensor(d_model, dtype=torch.float32)\n        self.warmup_steps = warmup_steps\n        self.step = 1.\n\n    def __call__(self):\n        arg1 = self.step ** -0.5\n        arg2 = self.step * (self.warmup_steps ** -1.5)\n        self.step += 1.\n        return (self.d_model ** -0.5) * min(arg1, arg2)\n\n\ndef train_model(config):\n    data_loader = LoadSentenceClassificationDataset(config.train_corpus_file_paths,\n                                                    my_tokenizer,\n                                                    batch_size=config.batch_size,\n                                                    min_freq=config.min_freq,\n                                                    max_sen_len=config.max_sen_len)\n    \n    train_iter, test_iter = data_loader.load_train_val_test_data(config.train_corpus_file_paths, config.test_corpus_file_paths)\n    \n    ## 定义模型并初始化权重\n    classification_model = ClassificationModel(vocab_size=len(data_loader.vocab),\n                                               d_model=config.d_model,\n                                               nhead=config.num_head,\n                                               num_encoder_layers=config.num_encoder_layers,\n                                               dim_feedforward=config.dim_feedforward,\n                                               dim_classification=config.dim_classification,\n                                               num_classification=config.num_class,\n                                               dropout=config.dropout)\n\n    for p in classification_model.parameters():\n        if p.dim() > 1:\n            nn.init.xavier_uniform_(p)\n    model_save_path = os.path.join(config.model_save_dir, 'model.pt')\n    if os.path.exists(model_save_path):\n        loaded_paras = torch.load(model_save_path)\n        classification_model.load_state_dict(loaded_paras)\n        print(\"## 成功载入已有模型，进行追加训练......\")\n    classification_model = classification_model.to(config.device)\n    \n    ## 定义损失学习率与优化器\n    loss_fn = torch.nn.CrossEntropyLoss()\n    learning_rate = CustomSchedule(config.d_model)\n    optimizer = torch.optim.Adam(classification_model.parameters(),\n                                 lr=0.,\n                                 betas=(config.beta1, config.beta2),\n                                 eps=config.epsilon)\n    classification_model.train()\n    max_test_acc = 0\n    \n    ## 开始训练\n    for epoch in range(config.epochs):\n        losses = 0\n        start_time = time.time()\n        for idx, (sample, label) in enumerate(train_iter):\n            sample = sample.to(config.device)  # [src_len, batch_size]\n            label = label.to(config.device)\n            \n            ## 生成每个样本对应的 padding mask 向量\n            padding_mask = (sample == data_loader.PAD_IDX).transpose(0, 1)\n            \n            logits = classification_model(sample,src_key_padding_mask=padding_mask)  # [batch_size,num_class]\n            optimizer.zero_grad()\n            loss = loss_fn(logits, label)\n            loss.backward()\n            lr = learning_rate()\n            for p in optimizer.param_groups:\n                p['lr'] = lr\n            optimizer.step()\n            losses += loss.item()\n\n            acc = (logits.argmax(1) == label).float().mean()\n            if idx % 10 == 0:\n                print(f\"Epoch: {epoch}, Batch[{idx}/{len(train_iter)}], \"\n                      f\"Train loss :{loss.item():.3f}, Train acc: {acc:.3f}\")\n        end_time = time.time()\n        train_loss = losses / len(train_iter)\n        print(f\"Epoch: {epoch}, Train loss: {train_loss:.3f}, Epoch time = {(end_time - start_time):.3f}s\")\n        if (epoch + 1) % config.model_save_per_epoch == 0:\n            acc = evaluate(test_iter, classification_model, config.device)\n            print(f\"Accuracy on test {acc:.3f}, max acc on test {max_test_acc:.3f}\")\n            if acc > max_test_acc:\n                max_test_acc = acc\n                torch.save(classification_model.state_dict(), model_save_path)\n\n\ndef evaluate(data_iter, model, device):\n    model.eval()\n    with torch.no_grad():\n        acc_sum, n = 0.0, 0\n        for x, y in data_iter:\n            x, y = x.to(device), y.to(device)\n            logits = model(x)\n            acc_sum += (logits.argmax(1) == y).float().sum().item()\n            n += len(y)\n        model.train()\n        return acc_sum / n","metadata":{"execution":{"iopub.status.busy":"2022-08-12T08:52:14.350765Z","iopub.execute_input":"2022-08-12T08:52:14.351074Z","iopub.status.idle":"2022-08-12T08:52:14.368734Z","shell.execute_reply.started":"2022-08-12T08:52:14.351047Z","shell.execute_reply":"2022-08-12T08:52:14.367790Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"markdown","source":"### **4、开始进行训练**","metadata":{}},{"cell_type":"code","source":"config = Config()\ntrain_model(config)","metadata":{"execution":{"iopub.status.busy":"2022-08-12T08:52:14.524002Z","iopub.execute_input":"2022-08-12T08:52:14.524509Z","iopub.status.idle":"2022-08-12T09:21:02.856529Z","shell.execute_reply.started":"2022-08-12T08:52:14.524481Z","shell.execute_reply":"2022-08-12T09:21:02.855422Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":22,"outputs":[]}]}