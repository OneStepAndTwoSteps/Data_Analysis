# `attention和tansformer`

## `transformer`


* 自注意力机制就是通过某种运算来直接计算得到句子在编码过程中每个位置上的注意力权重；然后再以权重和的形式来计算得到整个句子的隐含向量表示。最终，`Transformer` 架构就是基于这种的自注意力机制而构建的  `Encoder-Decoder` 模型。

    ![transformer-model-architecture](pic/transformer-model-architecture.jpg)

## `、注意力机制：`



## `、多头注意力：`

* `克服模型在对当前位置的信息进行编码时，会过度的将注意力集中于自身的位置的问题`

## 编码过程：






## `参考：`

* `Self-Attention和Transformer：`https://luweikxy.gitbook.io/machine-learning-notes/self-attention-and-transformer#transformer-wei-he-you-yu-rnn

* `详解Transformer （Attention Is All You Need）：`https://zhuanlan.zhihu.com/p/48508221