# Onehot编码：

## `机器学习算法 无法处理类别型变量，必须转换为数值型变量`

几乎所有特征都可以分为两个类别： __连续型特征和离散型特征。__

数据处理时会用到以下的知识：

__连续性特征使用归一化。__

    特征A的取值范围是[-1000,1000]，特征B的取值范围是[-1,1]。
    如果使用logistic回归，w1x1+w2x2，因为x1的取值太大了，所以x2基本起不了作用。
    所以，必须进行特征的归一化，每个特征都单独进行归一化。

__离散型使用独热编码/哑变量。__

    离散特征的编码分为两种情况：

    1、离散特征的取值之间没有大小的意义，比如color：[red,blue],那么就使用one-hot编码

    2、离散特征的取值有大小的意义，比如size:[X,XL,XXL],那么就使用数值的映射{X:1,XL:2,XXL:3}


__在实际的应用场景中，有非常多的特征不是连续的数值变量，而是某一些离散的类别。__ 比如用户的性别，用户的地址，用户的兴趣爱好等等一系列特征，都是一些分类值。这些特征一般都无法直接应用在需要进行数值型计算的算法里。这个时候我们使用独热编码。

__需要注意的是：__ 独热编码只适用于定类变量（Nominal variables），即属性与属性之间不能排序。比如「性别」中1表示男性、2表示女性，这种情况下数字1和数字2之间是平等的、并不存在大小关系，这种情况下使用独热编码是正确的。


## `什么是Onehot编码`

<div align="center"><img width="800" height="300" src="./static/onehot.png"/></div>

首先我们根据最左侧的表格对比我们的Binary和我们的One-hot

Binarys 根据我们的二进制进行一个表示的 000 表示0  ，001表示1，010表示2 ……  这种编码方式的好处在于，我们可以尽可能的对我们的编码方式进行一个压缩，同样的一个信息我们使用我们这种编码方式可能占用的位数尽量的少。
 
One-hot的编码方式：我们可以假设我们的One-hot是一个字典，这里面的数字只有一个数字是1，是1的这个数组相当于我们的标签中的索引，00000001 表示0 00000010 表示1 即第几个数字为1，那表示的就是它对应的位置减一。

One-hot的其中一个应用就是字典：
比如我们现在有 单词 man woman boy girl等存在字典中，这些单词单独存在是难以创建索引的，一种方式就是我们的one-hot编码，比如我们这里有9个单词，我们现在创建一个长度为9的向量做one-hot编码表示，每一个单词都可以得到一个1*9这样的向量表达，这个时候我们只要读取到我们的one-hot编码中哪一个状态位为1，那么他就表示哪一个单词。(上面的1-9就是我们的key 左边的单词就是我们的value)

这样有什么好处呢，好处是可以和我们的softmax的概率进行一个结合 这个后面再展开

在MINIST数据集中我们使用one-hot可以帮助我们进行损失值的计算，实际上我们在训练神经网络语言模型时，我们也需要对语言词汇表进行一个表示，此时我们使用one-hot词汇表就不太适用，加入现在我们的词汇表中有6000个单词，那我们就需要存储6000个向量，这往往是没有必要的。因为我们的单词之间可能是有关系的，使用one-hot则表示我们的单词和单词之间完全没有关系，都是独立的。

### `为什么要进行Onehot编码`

因为我们不能直接对数据(无序数据)进行数字映射，举个例子：

我们现在有一个无序特征(color), green  blue  red 三种颜色，现在如果我们将green映射为1，red映射为2，blue映射为3，然后进行训练的时候学习算法会认为‘green’比‘blue’大，‘red’比‘green’大。而这显然是不正确的，因为本身颜色是无序的！模型错误的使用了颜色特征信息，最后得到的结果肯定不是我们想要的，并且我们创造这个映射的原因也只是想用于表示，而不想其具备顺序条件。

__因为，分类器往往默认数据数据是连续的（可以计算距离？），并且是有序的（而上面这个0并不是说比1要高级）__


常用的解决方法为独热编码，对于‘颜色’这一特征中的‘蓝色’，我们将其编码为[蓝色=1，绿色=0，红色=0]，同理，对于‘绿色’，我们将其编码为[蓝色=0，绿色=1，红色=0]，特点就是向量只有一个1，其余均为0，故称之为one-hot。


为什么要进行Onehot编码呢？__因为大部分算法是基于向量空间中的度量来进行计算的__，为了使非偏序关系的变量取值不具有偏序性，并且到圆点是等距的。使用one-hot编码，将离散特征的取值扩展到了欧式空间，离散特征的某个取值就对应欧式空间的某个点。将离散型特征使用one-hot编码，会让特征之间的距离计算更加合理。

离散特征进行one-hot编码后，编码后的特征，其实每一维度的特征都可以看做是连续的特征。就可以跟对连续型特征的归一化方法一样，对每一维特征进行归一化。比如归一化到[-1,1]或归一化到均值为0，方差为1。

__举个例子：__

__将离散型特征使用one-hot编码，可以会让特征之间的距离计算更加合理。__ 比如，有一个离散型特征，代表工作类型，该离散型特征，共有三个取值，不使用one-hot编码，其表示分别是x_1 = (1), x_2 = (2), x_3 = (3)。两个工作之间的距离是，(x_1, x_2) = 1, d(x_2, x_3) = 1, d(x_1, x_3) = 2。那么x_1和x_3工作之间就越不相似吗？显然这样的表示，计算出来的特征的距离是不合理。那如果使用one-hot编码，则得到x_1 = (1, 0, 0), x_2 = (0, 1, 0), x_3 = (0, 0, 1)，那么两个工作之间的距离就都是sqrt(2).即每两个工作之间的距离是一样的，显得更合理。

__注意⚠️：__

为什么特征向量要映射到欧式空间？

* 将离散特征通过one-hot编码映射到欧式空间，是因为，在回归、分类、聚类等机器学习算法中，特征之间距离的计算或相似度的计算是非常重要的，而我们常用的距离或相似度的计算都是在欧式空间的相似度计算。

### `独热编码优缺点`

* __优点：__ 独热编码解决了分类器不好处理属性数据的问题，在一定程度上也起到了扩充特征的作用。它的值只有0和1，不同的类型存储在垂直的空间。

* __缺点：__ 当类别的数量很多时，特征空间会变得非常大。在这种情况下，一般可以用PCA来减少维度。而且one hot encoding+PCA这种组合在实际中也非常有用。



### `onehot编码的使用场景`

#### 用：

* 独热编码用来解决类别型数据的离散值问题，


#### 不用：

* 热编码用来解决类别型数据的离散值问题。将离散型特征进行one-hot编码的作用，是为了让距离计算更合理，但如果特征是离散的，并且不用one-hot编码就可以很合理的计算出距离，那幺就没必要进行one-hot编码，比如，该离散特征共有1000个取值，我们分成两组，分别是400和600,两个小组之间的距离有合适的定义，组内的距离也有合适的定义，那就没必要用one-hot 编码。

* __基于树的方法是不需要进行特征的归一化，例如随机森林，bagging 和 boosting等。有些基于树的算法在处理变量时，并不是基于向量空间度量，数值只是个类别符号，即没有偏序关系，所以不用进行独热编码。__ Tree Model不太需要one-hot编码： 对于决策树来说，one-hot的本质是增加树的深度。决策树是没有特征大小的概念的，只有特征处于他分布的哪一部分的概念。

### `什么情况下(不)需要归一化`

#### 需要： 

* 基于参数的模型或基于距离的模型，都是要进行特征的归一化。


#### 不需要：

* 基于树的方法是不需要进行特征的归一化，例如随机森林，bagging 和 boosting等。


## `扩展：`

* Python 机器学习：https://ljalphabeta.gitbooks.io/python-/content/categorical_data.html
