
## 处理机器学习中的不平衡分类问题

在信用卡诈骗案例中，我们将诈骗信息和正常交易信息进行可视化显示，我们可以发现正常交易的交易数远远大于诈骗交易的交易数。这样就会影响模型的输出。

### 什么时候我们应该处理不平衡问题

    1、如果原数据分布你的理解应该是均衡的，但是你的训练数据不均衡，那么就要考虑不平衡样本处理。
    
    2、如果原数据分布不是均衡的，但是你的训练集的类别样本分布和原数据分布相同，那么可以不考虑分布不平衡的问题。如果两者相差较大，还是要做处理。

### 为什么类别不平衡会影响模型的输出

许多模型的输出类别是基于阈值的，例如逻辑回归中小于0.5的为反例，大于则为正例。在数据不平衡时，默认的阈值会导致模型输出倾向与类别数据多的类别，体现在模型整体的准确率很高。

对于极不均衡的分类问题，比如仅有1%的人是诈骗交易，99%的人是正常交易，最简单的分类模型就是将所有交易都划分为正常交易，模型都能得到99%的准确率，显然这样的模型并没有提供任何的信息。

在类别不平衡的情况下，对模型使用F值或者AUC值是更好的选择。

处理不平衡数据，可以从两方面考虑：

    一、是改变分类算法，在传统分类算法的基础上对不同类别采取不同的 加权方式 ，使得模型更看重少数类。

    二、是 改变数据分布 ，从数据层面使得类别更为平衡；

__一、对于权重法__

    权重法是比较简单的方法，我们可以对训练集里的每个类别加一个权重class weight。
    如果该类别的样本数多，那么它的权重就低，反之则权重就高。
    如果更细致点，我们还可以对每个样本加权重sample weight，思路和类别权重也是一样，即样本数多的类别样本权重低，反之样本权重高。
    sklearn中，绝大多数分类算法都有class weight和 sample weight可以使用。

__二、对数据层面的一些方法，改变数据分布的方法 主要是以下几种重采样__

    1）过采样：增加少数类样本的数量

    2）欠采样：减少多数类样本的数量

    3）综合采样：将过采样和欠采样结合


两种方法的先后顺序：我们一般都是 __先采用权重法__，如果权重法做了以后发现预测效果还不好，__再考虑采样法。__



### 这里我们先提出两种解决方案也是数据分析中最常用的两种方法，下采样和过采样！

### 随机欠采样（下采样）

__随机欠采样（下采样）的目标是通过随机地消除占多数的类的样本来平衡类分布；直到多数类和少数类的实例实现平衡，目标才算达成。__

__随机下采样的优点：__
    
    它可以提升运行时间；并且当训练数据集很大时，可以通过减少样本数量来解决存储问题。

__随机下采样的缺点：__
    
    它会丢弃对构建规则分类器很重要的有价值的潜在信息。

    被随机欠采样选取的样本可能具有偏差。它不能准确代表大多数。从而在实际的测试数据集上得到不精确的结果。

### 随机过采样（Over-Sampling）

__随机过采样通过随机复制少数类来增加其中的实例数量，从而可增加样本中少数类的代表性。__

__随机过采样的优点：__

    与欠采样不同，这种方法不会带来信息损失。

    表现优于欠采样。

__随机过采样的缺点：__
    
    由于复制少数类事件，它加大了过拟合的可能性。


### 信息性过采样：合成少数类过采样技术（SMOTE）: 
 
 SMOTE（Synthetic minoritye over-sampling technique,SMOTE）是Chawla在2002年提出的过抽样的算法，一定程度上可以避免数据不平衡的问题。可以参考
    
 __正负样本分布:__
 
   <div align=center><img width="600" height="350" src="https://raw.githubusercontent.com/OneStepAndTwoSteps/data_mining_analysis/master/static/%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B/%E6%95%B0%E6%8D%AE%E4%B8%8D%E5%B9%B3%E8%A1%A1/1.png"/></div>  
   
 很明显的可以看出，蓝色样本数量远远大于红色样本，在常规调用分类模型去判断的时候可能会导致之间忽视掉红色样本带了的影响，只强调蓝色样本的分类准确性，这边需要增加红色样本来平衡数据集。

 为了让数据平衡，我们采用SMOTE算法对少数类A进行过采样， __SMOTE算法过程是这样的：__
 
 __假设A类样本数为N,对于每个样本x:__
 
 （1）在N个样本中取x(i)的的k个近邻点，从中 __随机选一个点x(j)__,，在x(i)和x(j)之间插入一个点x(i)1作为新样本(x(i)1=x(i)+λ|x(j)-x(i)|,0<λ<1);
 
  <div align=center><img width="600" height="350" src="https://raw.githubusercontent.com/OneStepAndTwoSteps/data_mining_analysis/master/static/%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B/%E6%95%B0%E6%8D%AE%E4%B8%8D%E5%B9%B3%E8%A1%A1/3.png"/></div>  

 
 （2）将步骤（1）重复N次，就得到M个新样本（x(i)1,x(i)2.....x(i)N)
 
 （3）对M个样本全部执行（1）（2）操作，最后将总共得到M * N个新样本
 
### 改进的合成少数类过采样技术（MSMOTE）
之后补充

## 算法集成技术

上述部分涉及通过重采样原始数据提供平衡类来处理不平衡数据，在本节中，我们将研究一种替代方法：修改现有的分类算法，使其适用于不平衡数据集。

集成方法的主要目的是提高单个分类器的性能。该方法从原始数据中构建几个两级分类器，然后整合它们的预测。

###  基于 Bagging 的方法
之后补充
### 基于 Boosting 的方法
之后补充
### 自适应 boosting——AdaBoost
之后补充
### 梯度树 boosting
之后补充
### XGBoost



