# 特征工程实施步骤

数据和特征决定了机器学习的上限，而模型和算法只是逼近这个上限而已。那特征工程到底是什么呢？顾名思义，其本质是一项工程活动，目的是最大限度地从原始数据中提取特征以供算法和模型使用。

## 一、数据清洗

拿到原始数据以后，对原始数据进行清洗时非常重要的步骤。因为获得的原始数据里面有非常多的脏数据甚至错误数据，如果不对这些数据进行处理，会极大地影响最后模型的效果。所以数据清洗时非常重要的一个步骤。

### 1.异常值分析过滤

顾名思义，异常值分析过滤是分析检验数据中是否有错误数据或者不合理的数据。如果有，则将这些数据剔除。常见的异常值分析方法有;

*   __1)简单统计量分析方法__

    可以对变量做一个描述性的统计与分析，然后查看数据是否合理。例如比较常用的统计量包括最大值与最小值，如果变量超过了最大值最小值的范围，那这个值就为异常值。例如年龄属性，如果某人填写为200或者-1，这显然都是属于异常值的范畴。

*   __2)3σ原则__

    如果数据的分布服从高斯分布(正态分布)，3原则是指，测量值如果与平均值的偏差超过3σ3σ，即为异常值。原理很简单，但是非常实际，三个标准差以外的数据就可以认为是异常值了。另外，同学们请对一下三个数字敏感：0.683,0.954,0.997。-([正态分布3σ原则](https://github.com/OneStepAndTwoSteps/Data_Analysis/blob/master/%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B/%E6%AD%A3%E6%80%81%E5%88%86%E5%B8%833%CF%83%E5%8E%9F%E5%88%99.md))



### 2.数据类型检查

这一个步骤能避免后续出现的很多问题。例如年龄这个属性，应该全是数值类型。但是很多时候这个字段出现了字符串类型的值，很明显这就是异常值，需要进行相应的处理。比如根据身份证号来进行计算，或者给个特殊的值-99来标识等等。

### 3.清洗换行符制表符空格等特殊字符

如果原始数据某些字段中存在换行符空格制表符等特殊字符，绝大部分情况下会影响后面进一步的分析。所以在数据清洗阶段，根据业务需求处理掉这些特殊字符是很有必要的。例如在大部分场景中，清洗掉字符串中的换行符，都是很必要的。

## 二、数据预处理

*   __1.数据平滑__

    因为现在机器学习的主流是统计机器学习，既然是统计，自然就离不开概率的计算。例如在对文本进行分类时，语料库毕竟是有限的。假设w1w1,w2w2,w3w3没在语料库中出现过，那根据最大似然估计MLE，这些词出现的概率为0。但是实际上这些词出现的概率肯定是不为0的。像最大似然估计里涉及到很多概率的连乘计算，如果一个概率为0，就会导致整体计算结果为0。这时候，就需要我们对数据进行平滑了。 

    平滑的算法有很多。最简单的平滑方式属于加1平滑了，就是给每种情况出现的次数都加上1，这样就避免了概率为0的情况。这种方式简单粗暴，实际使用的效果一般也不会特别理想。当然还有Good-turning平滑，线性插值平滑（Linear Interpolation Smoothing）等其他算法，根据具体的业务场景进行选择。


*   __2.缺失值处理__

    __方法一：丢弃__

    如果某条样本的大部分字段都缺失，那么很明显这个样本就是不可用状态。这种处理方式简单粗暴，效率高，但是很明显适用范围有限，只适合数据缺失较少的情况。如果某个特征特别重要，数据缺失情况还特别严重，只能重新采集数据。

    __方法二：填充__

    相对丢弃而言，补全是更加常用的缺失值处理方式，通过一定的方法将缺失的数据补上，从而形成完整的数据记录对于后续的数据处理、分析和建模至关重要。常用的补全方法包括：

    *   __统计法：__ 首先我们会看是该特征是连续值还是离散值。如果是连续值，那么一般有两种选择，一是选择所有有该特征值的样本，然后取平均值，来填充缺失值，另一种是取中位数来填充缺失值。如果是离散值，则一般会选择所有有该特征值的样本中最频繁出现的类别值，来填充缺失值，当然最准确的还是通过将多个特征进行组合取出最合适的特征进行填充。

    *   __模型法：__ 我们会根据完整的数据作为训练数据，缺失值作为目标数据，建立模型，预测缺失数据，从而获得最可能的补全值。

    *   __专家补全：__ 对于少量且具有重要意义的数据记录，专家补足也是 非常重要的一种途径。
    
    *   __其他方法：__ 例如随机法、特殊值法、多重填补等。

    __方法三：真值转换__

    如果存在一个特征有缺失值，但是舍不得丢弃，我们可以采用真值转换的方法，举个例子：当我们现在 的性别特征存在缺失值，那么我们将选择将其中的值，包括男、女、未知从一个变量的多个值分布状态转换为多个变量的真值分布状态。__(就是进行 onehot编码)__

    *   转换前：性别（值域：男、女、未知）。
    *   转换后：性别_男（值域1或0）、性别_女（值域1或0）、性别_未知（值域1或0）。然后将这3列新的字段作为输入维度用以替换原来的1个字段参与后 续模型计算。


    __方法四：不处理__

    很多模型对于缺失值有容忍度或灵活的处理方法，因此在预处理阶段可以不做处理，像一些模型他是可以自行进行处理缺失值，如：KNN、决策树和随机森林、神经网络和朴素贝叶斯、DBSCAN（基于密度的带有噪声的空间聚类）等
    
    *   忽略，缺失值不参与距离计算，例如KNN。
    *   将缺失值作为分布的一种状态，并参与到建模过程，例如各种决 策树及其变体。
    *   不基于距离做计算，因此基于值的距离做计算，本身的影响就消除，例如DBSCAN。


*   __3.特殊特征的处理__

    对于时间原始特征，处理方法有很多，这里只举例几种有代表性的方法。　第一种是使用连续的时间差值法，即计算出所有样本的时间到某一个未来时间之间的数值差距，这样这个差距是UTC的时间差，从而将时间特征转化为连续值。第二种方法是根据时间所在的年，月，日，星期几，小时数，将一个时间特征转化为若干个离散特征，这种方法在分析具有明显时间趋势的问题比较好用。第三种是权重法，即根据时间的新旧得到一个权重值。比如对于商品，三个月前购买的设置一个较低的权重，最近三天购买的设置一个中等的权重，在三个月内但是三天前的设置一个较大的权重。当然，还有其他的设置权重的方法，这个要根据要解决的问题来灵活确定。

    对地理特征，比如“广州市天河区XX街道XX号”，这样的特征我们应该如何使用呢？处理成离散值和连续值都是可以的。如果是处理成离散值，则需要转化为多个离散特征，比如城市名特征，区县特征，街道特征等。但是如果我们需要判断用户分布区域，则一般处理成连续值会比较好，这时可以将地址处理成经度和纬度的连续特征。

*   __4.处理不平衡数据__

    -([在这篇文章中我总结了 如何处理不平衡数据](https://github.com/OneStepAndTwoSteps/Data_Analysis/blob/master/%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B/%E5%A4%84%E7%90%86%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%AD%E7%9A%84%E6%95%B0%E6%8D%AE%E4%B8%8D%E5%B9%B3%E8%A1%A1%E7%9A%84%E5%88%86%E7%B1%BB%E9%97%AE%E9%A2%98/readme.md))


*   __5.标准化、数据归一化__
    
    -([在这篇文章中我总结了 标准化、数据归一化](https://github.com/OneStepAndTwoSteps/Data_Analysis/tree/master/%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B/%E7%89%B9%E5%BE%81%E9%A2%84%E5%A4%84%E7%90%86))


*   __6.离散化__

    __有很多机器学习算法只能处理数值特征，不能处理离散值特征，比如线性回归，逻辑回归等。那么想使用逻辑回归，线性回归时这些值只能丢弃吗？当然不是。我们可以将离散特征连续化处理。__

    最常见的离散特征连续化的处理方法是独热编码 __([onehot-encoding](https://github.com/OneStepAndTwoSteps/Data_Analysis/tree/master/%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B/%E7%89%B9%E5%BE%81%E9%A2%84%E5%A4%84%E7%90%86/onehot%E7%BC%96%E7%A0%81))__。处理方法其实比较简单，比如某特征的取值是高，中和低，那么我们就可以创建三个取值为0或者1的特征，将高编码为1,0,0这样三个特征，中编码为0,1,0这样三个特征，低编码为0,0,1这样三个特征。也就是说，之前的一个特征被我们转化为了三个特征。sklearn的OneHotEncoder可以帮我们做这个处理。

    __例子：__ 而且有时候因为我们的数据中存在噪音，我们可以使用离散化很好的降低我们的噪音，在 kaggle的《泰坦尼克获救》中，我们发现，人们的年龄存在的值有很多，如果我们不进行处理直接进行分析的话，就会带来噪音，采用的方法可以将这些值进行离散化，比如我们可以将年龄划分为 1，2，3，4，5 五个等级 第一个等级代表年龄为 0-20，第二个等级代表年龄为21-40....，因为我们最终要进行分析的是是否年龄会给我们获救带来影响，我们将其划分为这5个等级其实也就是想表达年龄的高低，所以很适合采用离散化。

    这里我们还要提醒一下的是， __当我们给离散特征进行独热编码处理，处理后的特征就是连续值了__ ，onehot将离散特征变成 0 或者1 后，可以按连续特征的思路来做处理。所以你可以这么理解：特征onehot后变到了[0,1]区间，所以是连续特征了。
    
    使用 sklearn 中的 onehotencoding 或者 pandas 中的 get_dummies 都可进行 onehot 编码
    
*   __7.dummy coding__
    
    它和独热编码类似，但是它的特点是，如果我们的特征有N个取值，它只需要N-1个新的0,1特征来代替，而独热编码会用N个新特征代替。比如一个特征的取值是高，中和低，那么我们只需要两位编码，比如只编码中和低，如果是1，0则是中，0,1则是低。0,0则是高了。目前虚拟编码使用的没有独热编码广，因此一般有需要的话还是使用独热编码比较好。

## 三、[特征选择](https://github.com/OneStepAndTwoSteps/Data_Analysis/tree/master/%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B/%E7%89%B9%E5%BE%81%E9%80%89%E6%8B%A9)


*   __1.embedded 嵌入类方法__

     在嵌入式特征选择中，特征选择算法本身作为组成部分嵌入到学习算法里最典型的即决策树算法，如ID3、C4.5以及CART算法等，决策树算法在树增长过程的每个递归步都必须选择一个特征，将样本集划分成较小的子集，选择特征的依据通常是划分后子节点的纯度，划分后子节点越纯，则说明划分效果越好，可见决策树生成的过程也就是特征选择的过程。


*   __2.wrapper 包装类方法__

    封装式特征选择是利用学习算法的性能评价特征子集的优劣。因此，对于一个待评价的特征子集， Wrapper方法需要训练一个分类器，根据分类器的性能对该特征子集进行评价Wrapper方法中用以评价特征的学习算法是多种多样的，例如决策树、神经网络、贝叶斯分类器、近邻法、支持向量机等等。

    __优点：__ 相对于Filter方法，Wrapper方法找到的特征子集分类性能通常更好。

    __缺点：__ Wrapper方法选出的特征通用性不强，当改变学习算法时，需要针对该学习算法重新进行特征选择；由于每次对子集的评价都要进行分类器的训练和测试，所以算法计算复杂度很高，尤其对于大规模数据集来说，算法的执行时间很长。

    __常见的包装法有：__([RFE](https://github.com/OneStepAndTwoSteps/Data_Analysis/blob/master/%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B/%E7%89%B9%E5%BE%81%E9%80%89%E6%8B%A9/%E5%8C%85%E8%A3%85%E6%B3%95-RFECV.md))

*   __3.filter 过滤类方法__

    过滤式特征选择的评价标准从数据集本身的内在性质获得，与特定的学习算法无关，因此具有较好的通用性。通常选择和类别相关度大的特征或者特征子集。过滤式特征选择的研究者认为，相关度较大的特征或者 特征子集会在分类器上获得较高的准确率。过滤式特征选择的评价标准分为四种，即距离度量、信息度量、关联度度量以及一致性度量。

    __优点：__ 算法的通用性强；省去了分类器的训练步骤，算法复杂性低，因而适用于大规模数据集；可以快速去除大量不相关的特征，作为特征的筛选器非常合适。

    __缺点：__ 甶于算法的评价标准独立于特定的学习算法，所选的特征子集在分类准确率方面通常低于Wrapper方法。

    __常见的过滤法有：__([方差选择法](https://github.com/OneStepAndTwoSteps/Data_Analysis/blob/master/%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B/%E7%89%B9%E5%BE%81%E9%80%89%E6%8B%A9/%E5%8C%85%E8%A3%85%E6%B3%95-RFECV.md))

## 四、特征组合

严格意义上来说，特征组合也属于特征选择的一部分。取工业界最常见的LR模型为例，LR模型本质上是广义线性模型(对数线性模型)，实现简单而且容易并行，计算速度也比较快，同时使用的特征比较好解释，预测输出的概率在0与1之间也非常方便易用。但是，与一般模型的容易overfitting不一样，LR模型却是一个underfitting模型，因为LR模型本身不够复杂，甚至可以说相当简单。而现实中很多问题不仅仅是线性关系，更多是复杂的非线性关系。这个时候，我们就希望通过特征组合的方式，来 __描述这种更为复杂的非线性关系。__

*   __特征组合的种类__

    我们可以创建很多不同种类的特征组合。例如：

        [A X B]：将两个特征的值相乘形成的特征组合。

        [A x B x C x D x E]：将五个特征的值相乘形成的特征组合。

        [A x A]：对单个特征的值求平方形成的特征组合。

    通过采用随机梯度下降法，可以有效地训练线性模型。因此，在使用扩展的线性模型时辅以特征组合一直都是训练大规模数据集的有效方法。

#### 目前常见的用于特征组合的方法：

*   __1.GBDT__

*   __2.FM__



## 五.数据降维

对应数据降维，主要分为线性降维和非线性降维，我将通过人脸识别的例子进描述：

*   __1.线性降维__

    在研究早期，人们使用子空间算法：将人脸图像当作一个高维向量， 将其投影到低维空间后，期望得到的低维向量对不同的人具有区分度。 
    比如Matthew等使用PCA降维得到特征脸（Eigenface )、Peter等使用LDA降维得到Fisherface来实现人脸识别。

    但是，PCA和LDA都是线性降维技术，显然人脸在高维空间中的分 布是非线性的。因此，我们可以进一步使用非线性降维算法。如：流 形和核方法。

    <div align=center><img width="300" height="300" src="https://raw.githubusercontent.com/OneStepAndTwoSteps/Data_Analysis/master/static/%E9%99%8D%E7%BB%B4PCA/11.png"/></div>


    在研究早期，我们使用子空间算法：什么意思呢？其实就是将人脸图像当作一个高维向量， 将其投影到低维空间后，期望得到的低维向量对不同的人具有区分度。 他其实本质上简化下来就是我们最开始使用全连接网络去学习我们的手写体识别，一开始我们使用的算法都是线性的算法，比如说Matthew提出可以使用PCA得到一种线性降维的方式降维到一个特征脸，你可以认为我们将向量进行一个标准化之后，我们对他的特征向量中较大的值保留下来，有一些比较小的值我们认为是一些冗余或者噪音信息，我们就把他丢失掉了，其实就保证了我们从高维讲到低维的时候，我们不同向量之间的相对位置还是比较一致的，右边的图就是我们PCA的降维图，不同的颜色就是我们不同的类别，之前在三维空间的时候我们有1000个小方块，降维到二维的时候就只有100个小方块了，当降维到一维的时候就只剩下了10个小方块，我们可以看到降维之后，不论是相对位置还是分类都仍然保留了信息。

    除了PCA之外还可以使用LDA这种线性降维的技术，Peter等人使用LDA降维得到Fisherface来实现人脸识别。但是效果也很一般。这是因为显然人脸在高维空间中的分布是非线性的，这样我们通过线性降维的方式得到的这些特征其实都没有很好的效果，因此我们可以进一步使用非线性降维算法。如：流 形和核方法。




*   __1.非线性降维__

    流形学习是一种非线性降维方法，它假设向量点在高维空间中的分布具有某些几何形状，我们先来看一下左图的一幅典型的瑞士卷的图像结构，瑞士卷的数据集如果我们使用线性降维的方法之后得到的一个低维的分布其实是非常糟糕的(非右图)，完全就看不出他在三维空间中所具有的特性，从我们的视角取看，在这个三维空间中瑞士卷数据集的分布其实是非常明确的，比如说以中间这个点为圆心，然后向外展开，最靠近圆心的一段是一种颜色(一种类别)，之后其他不同的颜色又是一种类别，但是我们如果直接使用PCA的话那么得到的分布可能就会比较糟糕，通过流形学习的话，保持这些几何形状约束的前提下将向量投影到低维空间中，这种投影是通过非线性变换完成的。我们的深度学习中的全连接网络就有这种非线性变化的能力。

    <div align=center><img width="800" height="350" src="https://raw.githubusercontent.com/OneStepAndTwoSteps/Data_Analysis/master/static/%E6%B5%81%E8%A1%8C%E5%AD%A6%E4%B9%A0/1.png"/></div>
    
    我们之后也可以通过神经网路学习非线性变化的过程，瑞士卷的例子就是尽可能的通过非线性降维的方式，降低我们降维时对数据分布的影响。我们可以看到图中我们将三维的图降维到二维时，我们的数据分布也得到了保证。

    但是这些通过直接降维来进行人脸识别的早期算法只是将我们原来的图像在我们的高维空间做了一个投影而已，虽然非线性降维可以保证我们的特征位置不受影响，但是却无法在其中学习到很多东西，只是对原来的投影数据做的一个简单的投影变化而已。普遍都存在严重依赖训练集和测试集场景的问题， 且对光照、人脸的表情、姿态敏感，泛化能力不足，不具有太多的实用价值。



*   __1.([主成分分析 Principal Component Analysis(PCA)](https://github.com/OneStepAndTwoSteps/Data_Analysis/blob/master/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E7%AE%97%E6%B3%95/%E9%99%8D%E7%BB%B4/readme.md))__

*   __2.奇异值分解 Singular Value Decomposition(SVD)__

*   __3.线性判别分析 Linear Discriminant Analysis(LDA)__

*   __4.流行学习 (TSNE)__

    常用于非线性降维可视化
    
#### 部分笔记参考 

-《[刘建平老师博客-特征表达](https://www.cnblogs.com/pinard/p/9061549.html#!comments)》

-《[用户特征工程 超详细解读](https://blog.csdn.net/bitcarmanlee/article/details/52244237)》

