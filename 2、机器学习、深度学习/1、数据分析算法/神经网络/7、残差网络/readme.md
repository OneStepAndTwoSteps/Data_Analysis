# `residual network`

## `残差模块解决了什么问题？`

* `一定程度上可以缓解梯度弥散问题：`

    现代神经网络一般是通过基于梯度的BP算法来优化，对前馈神经网络而言，一般需要前向传播输入信号，然后反向传播误差并使用梯度方法更新参数。

    根据链式法则，当导数<1时，会导致反向传播中梯度逐渐消失，底层的参数不能有效更新，这也就是梯度弥散(或梯度消失)；当 导数>1 时，则会使得梯度以指数级速度增大，造成系统不稳定，也就是梯度爆炸问题。此问题可以被标准初始化和中间层正规化方法有效控制，这些方法使得深度神经网络可以收敛。

* `一定程度上解决网络退化问题：`

    在神经网络可以收敛的前提下，随着网络深度增加，网络的表现先是逐渐增加至饱和，然后迅速下降。

    网络退化问题不是过拟合导致的，即便在模型训练过程中，同样的训练轮次下，退化的网络也比稍浅层的网络的训练错误更高，如下图所示。



    如果存在某个 K层的网络是当前最优的网络，那么可以构造一个更深的网络，其最后几层仅是该网络 f 第 K 层输出的恒等映射(Identity Mapping)，就可以取得与 f一致的结果；也许 K还不是所谓“最佳层数”，那么更深的网络就可以取得更好的结果。总而言之，与浅层网络相比，更深的网络的表现不应该更差。因此，一个合理的猜测就是，对神经网络来说，恒等映射并不容易拟合。

* `一定程度上缓解梯度破碎问题：`

    在标准前馈神经网络中，随着深度增加，梯度逐渐呈现为白噪声(white noise)。许多优化方法假设梯度在相邻点上是相似的，破碎的梯度会大大减小这类优化方法的有效性。另外，如果梯度表现得像白噪声，那么某个神经元对网络输出的影响将会很不稳定。


## `为什么残差模块可以解决这些问题？`


* `残差模块的结构:`
  
    残差网络通过加入 ，变得更加容易被优化。包含一个 `shortcut connection` 的几层网络被称为一个`残差块（residual block）`。

    一个`残差块（shortcut connections/skip connections）`分为直接映射部分$（x_l）$和残差部分 $F(x_l​,W_l​)$ ，可以表示为：

    $$x_{l+1}​=x_l​+F(x_l​,W_l​)$$

    `示意图如下图所示：`


    <div align=center><img  height="200" src="./static/residual_network.jpg"/></div>

* `残差模块如何解决上述问题？`


    `1、梯度弥散：`根据后向传播的链式法则可以看到，上述残差块中，因为增加了**x项(恒等映射)**，那么该网络求 x​ 的偏导的时候，多了一项常数 1，所以反向传播过程，梯度连乘，也不会造成梯度消失。


    <div align=center><img  height="250" src="./static/back_pro.jpg"/></div>


    `2、网络退化：`在前向传播时，输入信号可以从任意低层直接传播到高层。由于包含了一个天然的恒等映射，一定程度上可以解决网络退化问题。


    `3、梯度破碎：`
    
    <div align=center><img  height="550" src="./static/梯度破碎.jpg"/></div>


## `reference:`


* `Transformer相关——（5）残差模块：`https://ifwind.github.io/2021/08/17/Transformer%E7%9B%B8%E5%85%B3%E2%80%94%E2%80%94%EF%BC%885%EF%BC%89%E6%AE%8B%E5%B7%AE%E6%A8%A1%E5%9D%97/#%E6%AE%8B%E5%B7%AE%E6%A8%A1%E5%9D%97%E5%A6%82%E4%BD%95%E8%A7%A3%E5%86%B3%E4%B8%8A%E8%BF%B0%E9%97%AE%E9%A2%98