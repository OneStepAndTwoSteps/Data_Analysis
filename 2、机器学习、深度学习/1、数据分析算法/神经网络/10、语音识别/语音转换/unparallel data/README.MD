# `UNPARALLEL DATA VOICE CONVERSION:`

## `一些对于非对齐数据的voice conversion 的方法：`

* `1、设法构造并行数据：`


    对目标数据和源语音数据进行切分，再进行聚类，如果在一簇中则认为数据大致相同，是对应的数据，通过这样的方式在特征的维度上来进行对齐的操作，然后再通过映射函数来建立源语音到目标语音之间的映射。

* `2、基于ASR：`

    通过ASR系统(语音识别的系统)，先将wav的特征转成基于文本内容的特征，然后让基于文本内容的特征和目标的语音的vc特征进行对齐，进行映射，来学习本文到目标语音的转换。

    再接收测试数据的时候一样，先想测试数据转成对应的文本特征，然后再将文本特征转成目标语音。

    <div align=center><img height  src="./static/1.jpg"/></div>

* `3、基于VAE：`

    <div align=center><img height  src="./static/2.jpg"/></div>


    * 1.编码器：接收原始语音信号，将其转换为潜在变量z的分布参数（例如，均值和方差），并从中采样得到隐变量z。
    
    * 2.解码器：接收隐变量z，将其解码为目标语音信号。
    
    * 3.鉴别器：接收原始语音信号和目标语音信号，将它们映射到一个共享的隐藏层上，并对比这些编码的相似性。
    
    * 4.损失函数：VAE的损失函数通常由两部分组成，即重构损失和KL散度损失。重构损失用于评估解码器的输出和原始语音信号之间的差异，KL散度损失用于保证潜在变量z的分布与标准正态分布之间的差异最小化。
    
    在语音转换中，VAE的编码器可以将输入语音信号转换为一组潜在变量，例如说语音的声调、音调、语速、情感等。解码器可以将这些潜在变量转换为目标语音信号，使其具有与目标说话人相似的特征。鉴别器可以帮助VAE更好地学习说话人之间的差异，从而提高转换的质量。

    `补充：`

    * 隐变量是概率分布中的一个参数，而不是概率分布本身。引入隐变量的一个重要作用就是将全局的概率密度计算转换为局部的概率密度计算。在概率模型中，隐变量通常是指我们无法直接观测到的变量，但是它们对于模型的学习和预测是非常重要的。在VAE中，隐变量z是一个低维度的向量，它被认为是一个潜在的表示，包含了输入数据的重要特征(在voice conversion中如说话人的语音特征、情感状态、语速等等。)。VAE的目标是学习从输入数据到隐变量z的概率分布，并使用这个分布来采样隐变量z，然后再使用解码器将隐变量z转换为目标数据的分布，例如图像、语音等。

        在VAE中，我们假设隐变量z的分布是一个高斯分布，它由均值和方差两个参数来描述。编码器的任务就是从输入数据中提取出隐变量z的均值和方差，并根据这些参数来采样得到隐变量z。因此，隐变量z是一个由均值和方差两个参数组成的概率分布。在VAE的训练过程中，我们使用重构误差和KL散度来约束隐变量z的分布，使得它更加接近于标准正态分布。这样可以让我们更好地学习到数据的潜在表示，从而提高模型的泛化能力。


        在使用VAE进行语音转换时，我们首先需要使用编码器将输入语音信号转换为隐变量z的分布，通常是一个高斯分布。这个高斯分布的均值和方差是由编码器网络输出的，因此可以看作是后验概率密度函数p(z|x;θ)的近似分布。然后，我们从这个分布中采样得到具体的隐变量z，并使用解码器将其转换为目标说话人的语音信号。

        因为，隐变量z包含了原始语音信号的一些重要特征，所以它可以帮助我们将原始语音信号转换为具有目标说话人特征的语音信号。在训练过程中，我们通常会使用一些约束来限制隐变量z的分布，例如 KL散度等，以便于让隐变量z更好地表示语音信号的潜在特征。

    在encoder后做了 source 和 target 的分布的对齐。


* `4、基于Cycle GAN：`

    前述的非并行VC算法中，始终离不开“对齐”的操作，或者将音频特征切分后，在特征维度对齐或者对语音进行识别，在文本层级对齐。或者利用深度神经网络的编码功能在潜变量或者分布上进行对齐

    基于CycleGAN的VC算法，真正意义上的摆脱了对齐的操作，利用CycleGAN进行直接的非并行声音抓换，并且训练得到的网络可以实现A->B 与B->A的操作。

    <div align=center><img height  src="./static/3.jpg"/></div>




