## `implement sampling ：`
<br>

* 可以更详细的解释一下吗，从为什么on palicy 中的每一次更新Θ都需要重新采样的原因是为什么，如果每次更新后不重新采样会引入什么问题，然后implement sample 是一个什么样的技术，它是从哪个角度解决不需要每次更新参数都要重新采样的问题的，解决的方法在公式中的哪里体现等等：

<br>

* 首先，on policy 中的每一次更新Θ都需要重新采样的原因是，on policy 的方法要求采样所用的策略和目标策略一致，也就是说，用来收集数据的策略和用来学习和改进的策略是同一个。因此，当目标策略更新了，采样的策略也要同步更新，以保持一致性。这就导致了之前收集的数据可能不再适用于新的策略，所以需要重新采样。

    如果每次更新后不重新采样，会引入什么问题呢？一个可能的问题是，之前收集的数据可能不再覆盖新的策略可能产生的行为，也就是说，新的策略可能会在某些状态下选择不同的动作，但是之前收集的数据没有包含这些情况。这样就会导致数据偏差和方差增大，影响策略的评估和改进。

    那么，implement sampling 是一个什么样的技术呢？它是从哪个角度解决不需要每次更新参数都要重新采样的问题的呢？implement sampling 其实是 importance sampling 的中文翻译，它是一种利用一个概率分布的样本来估计另一个概率分布的期望值的方法。它是从 off policy 的角度解决这个问题的，也就是说，它允许采样所用的策略和目标策略不同，这样就可以避免每次更新参数都要重新采样。

    具体来说，假设我们有一个目标策略 $\pi$ 和一个行为策略 $\mu$ ，我们想要用 $\mu$ 产生的数据来估计 $\pi$ 对应的值函数或者优势函数。这时候，我们可以在对应的样本估计量上乘上一个重要性权重 $\rho$ ，这个权重是 $\pi$ 和 $\mu$ 的概率比例。例如，如果我们想要用 off policy 的方法来估计状态值 $V^\pi$ ，我们可以使用下面的公式：

    $$V^\pi (s)\approx\frac {1} {N}\sum_ {n=1}^N {\rho_ {t (n)}G_ {t (n)}}$$

    其中 N 是采样轨迹的数量，$t (n)$ 是第 n 条轨迹中状态 s 出现的时间点，$\rho_ {t (n)}$ 是从 t (n) 时刻开始到轨迹结束时刻 T (n) 的重要性权重累积积分：

    $$\rho_ {t (n)}=\prod_ {k=t (n)}^{T (n)}{\frac {\pi (A_k|S_k)} {\mu (A_k|S_k)}}$$

    $G_ {t (n)}$ 是从 t (n) 时刻开始到轨迹结束时刻 T (n) 的累积回报：

    $$G_ {t (n)}=\sum_ {k=t (n)}^{T (n)}{\gamma^{k-t (n)}R_k}$$

    这个公式就体现了 importance sampling 的方法，在 off policy 的情况下，用 $\mu$ 产生的数据来估计 $\pi$ 对应的状态值，并且用重要性权重来校正两个策略之间的差异。