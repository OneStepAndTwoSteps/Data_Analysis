# 数据规范化

### 离群点


离群点是一个或一组明显不同于其他数据的数据点。 Hawkins把 __离群点定义为“离群点是在数据集中偏离大部分数据的数据__ ，使人怀疑这些数据的偏离并非由随机因素产生，而是产生于完全不同的机制。”

在大多数应用中，数据由一到多个程序产生，这些数据可以反映出系统的运行状态和被监测客体的相关数值。当测量、输入错误或系统运行错误时，或者客体出现异常行为，离群点就会产生。因此，离群点经常包含关于系统与实体异常特征的有效信息。现有数据挖掘研究大多集中于发现适用于大部分数据的常规模式,在许多应用领域中，__离群点通常作为噪音而忽略__，许多数据挖掘算法试图降低或消除离群点的影响。__而在有些应用领域识别离群点是许多工作的基础和前提，离群点会带给我们新的视角__。如在欺诈检测中，离群点可能意味欺诈行为的发生，在入侵检测中离群点可能意味入侵行为的发生。其他的一些应用场景包括信用卡诈骗、医疗诊断、执法、地球科学……


-《[引用自：离群点挖掘简述](https://blog.chih.me/Outlier-mining-review.html)》


__注意：__

群点(outlier)是一个数据对象，它显著不同于其他数据对象，就像是被不同的机制产生一样，在样本空间中，与其他样本点的一般行为或特征不一致的点。__值得注意的是，离群点并不是异常值。__（比如说，A月薪50w，B、C、D月薪5000，虽然A月薪异常于样本集，是离群点，但是它并不是异常值。）


### StandardScaler

公式为：(X-mean)/std 计算时对每个属性/每列分别进行。
将数据按期属性（按列进行）减去其均值，并处以其方差。得到的结果是，__对于每个属性/每列来说所有数据都聚集在0附近，方差为1。__

       >>> from sklearn.preprocessing import StandardScaler
       >>> data = [[0, 0], [0, 0], [1, 1], [1, 1]]
       >>> scaler = StandardScaler()
       >>> print(scaler.fit(data))
       StandardScaler(copy=True, with_mean=True, with_std=True)
       >>> print(scaler.mean_)
       [0.5 0.5]
       >>> print(scaler.std_)
       [0.5 0.5]
       >>> print(scaler.transform(data))
       [[-1. -1.]
       [-1. -1.]
       [ 1.  1.]
       [ 1.  1.]]


       from sklearn.preprocessing import StandardScaler

       scaler = StandardScaler()
       train['ss_Fare'] = scaler.fit_transform(train[['Fare']])
       train.head()

### RobustScaler

如果数据有 __离群点__，上述StandardScaler效果可能不好，这种情况可以使用RobustScaler，它有对数据中心化和数据的缩放鲁棒性更强的参数
RobustScaler根据分位数范围（默认为IQR： IQR是第1四分位数和第3个四分位数之间的范围。）删除中位数并缩放数据。


       >>> from sklearn.preprocessing import RobustScaler
       >>> X = [[ 1., -2.,  2.],
       ...      [ -2.,  1.,  3.],
       ...      [ 4.,  1., -2.]]
       >>> transformer = RobustScaler().fit(X)
       >>> transformer
       RobustScaler(copy=True, quantile_range=(25.0, 75.0), with_centering=True,
              with_scaling=True)
       >>> transformer.transform(X)
       array([[ 0. , -2. ,  0. ],
              [-1. ,  0. ,  0.4],
              [ 1. ,  0. , -1.6]])



### MinMaxScaler


       from sklearn.preprocessing import MinMaxScaler

       scaler = MinMaxScaler(feature_range=(0, 1))  #自动将dtype转换成float64
       train['mm_Fare'] = scaler.fit_transform(train[['Fare']])
       train.head()
